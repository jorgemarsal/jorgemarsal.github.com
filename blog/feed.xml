<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jorge Martinez</title>
    <description></description>
    <link>http://jorgemarsal.github.io/blog/</link>
    <atom:link href="http://jorgemarsal.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 17 May 2015 20:55:15 -0700</pubDate>
    <lastBuildDate>Sun, 17 May 2015 20:55:15 -0700</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Design of a simple distributed application</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Today I’m going to explain how to design a simple distributed application. The goal is to read a really big file into memory. Since the file doesn’t fit in a single machine’s memory we need to design a system to split the file across different machines. A possible architecture comprises a &lt;code&gt;master&lt;/code&gt; that breaks the file into smaller chunks and &lt;code&gt;N workers&lt;/code&gt; that process those chunks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/usecase.png&quot; alt=&quot;Use case&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The master should implement the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A way to split the work across workers. Different policies like round-robin or sending to the worker with less requests in flight are possible.&lt;/li&gt;
  &lt;li&gt;A way of registering workers.&lt;/li&gt;
  &lt;li&gt;A way to monitor the workers, notice if they go down and send their pending splits to other workers.&lt;/li&gt;
  &lt;li&gt;A way to timeout if there are no workers registered or the operation takes too long.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Master and workers communicate using the following &lt;code&gt;protobuf&lt;/code&gt; messages over a &lt;code&gt;zeromq&lt;/code&gt; transport:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;FETCH_SPLIT_REQUEST: master -&amp;gt; worker. Asks the worker to fetch a split.&lt;/li&gt;
  &lt;li&gt;FETCH_SPLIT_RESPONSE: worker -&amp;gt; master. The worker communicates the result of the operation to the master.&lt;/li&gt;
  &lt;li&gt;HEARTBEAT_REQUEST: master -&amp;gt; worker. Send to the worker to make sure it is alive.&lt;/li&gt;
  &lt;li&gt;HEARTBEAT_RESPONSE: worker -&amp;gt; master. Send response back to the master.&lt;/li&gt;
  &lt;li&gt;REGISTRATION: worker -&amp;gt; master. Register worker into the master.&lt;/li&gt;
  &lt;li&gt;SHUTDOWN: master -&amp;gt; worker. Used to shut down the worker.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The application works as follows. First the workers register with the master and at that point the master starts sending split requests to them. At a periodic interval the master sends heartbeats to check that the workers are still alive. If a worker doesn’t reply to a heartbeat the master removes the worker and re-sends the outstanding requests to other workers that are alive.
This continues until all the splits are complete.&lt;/p&gt;

&lt;h3 id=&quot;master-architecture&quot;&gt;Master architecture&lt;/h3&gt;
&lt;p&gt;We divide the master into 2 components: &lt;code&gt;master&lt;/code&gt; and &lt;code&gt;transport&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/arch.png&quot; alt=&quot;Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;master&lt;/code&gt; is in charge of sending &lt;code&gt;Fetch Split Requests&lt;/code&gt;, sending &lt;code&gt;heartbeats&lt;/code&gt; and processing &lt;code&gt;responses&lt;/code&gt; from workers.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;transport&lt;/code&gt; is in charge of sending and receiving the messages.&lt;/p&gt;

&lt;p&gt;To make the design cleaner we run &lt;code&gt;master&lt;/code&gt; and &lt;code&gt;transport&lt;/code&gt; in different threads and communicate them using a thread-safe producer consumer queue. Using different threads makes the application more responsive. E.g. if the master is taking a long time generating requests or processing responses the transport can still send and receive messages. This is the master’s pseudocode:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;splits = splitFile(filename)
while (splits.size() &amp;gt; 0) {
  now = get_time();
  if ((now - lastSplitResponseTime) &amp;gt; timeoutInterval) errorAndExit();
  if ((now - lastheartbeatSendTime) &amp;gt; heartbeatSendInterval) sendHeartbeat();
  if ((now - lastHeartbeatCheckTime) &amp;gt; hearbeatCheckInterval) checkHeartbeats();
  if (requestQueue.hasFreeSlots()) sendSplit();
  if (responseQueue.hasValidData()) processResponse();
}
shutdown(workers);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the queues are not blocking. This allows us to keep sending requests even if there are no responses and viceversa.&lt;/p&gt;

&lt;p&gt;At a certain point in time the queues will look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/queues.png&quot; alt=&quot;Queues&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is just a very simple design. One possible problem is that the heartbeat requests/responses may be delayed if there are too many Fetch Splits requests/responses in the queue. Another option would be to have a separate queue with higher priority for heartbeats (i.e. &lt;code&gt;transport&lt;/code&gt; would always empty the heartbeat queue first).
Another option is to have a whole separate &lt;code&gt;zeromq&lt;/code&gt; socket for heartbeats.&lt;/p&gt;

&lt;h3 id=&quot;transport-architecture&quot;&gt;Transport architecture&lt;/h3&gt;

&lt;p&gt;The transport layer is responsible for sending and receiving messages. As in the master case is not blocking:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while (1) {
  hasResponses = poll_for_responses();
  if (hasResponses) responseQueue.add(response);
  if (requestqueue.hasValiData()) sendToWorker(data);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;worker-architecture&quot;&gt;Worker architecture&lt;/h3&gt;
&lt;p&gt;The worker is very simple. It receives requests, does the work and sends a response in a closed loop.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while (1) {
  req = block_for_request();
  if(req == shutdown) break;
  rsp = doWork(rep);
  sendToMaster(rsp);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The complete code is on &lt;a href=&quot;https://github.com/jorgemarsal/distributor&quot;&gt;Github&lt;/a&gt; in case you want to take a closer look ;).&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;When you have a big problem scaling out across a cluster of commodity machines can be a good option.&lt;/p&gt;

&lt;p&gt;In big clusters machines fail so we need a way to check for failures and react to them.&lt;/p&gt;

&lt;p&gt;In this example the master is not fault tolerant. We could implement a fault tolerant system using something like  &lt;a href=&quot;https://github.com/logcabin/logcabin&quot;&gt;Logcabin&lt;/a&gt; or &lt;a href=&quot;https://zookeeper.apache.org/&quot;&gt;Zookeeper&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Sun, 17 May 2015 00:00:00 -0700</pubDate>
        <link>http://jorgemarsal.github.io/blog/2015/05/17/design-of-a-simple-distributed-application.html</link>
        <guid isPermaLink="true">http://jorgemarsal.github.io/blog/2015/05/17/design-of-a-simple-distributed-application.html</guid>
        
        
      </item>
    
      <item>
        <title>Introducing Distributed R in the Silicon Valley Machine Learning Meetup</title>
        <description>&lt;p&gt;A few days ago we introduced Distributed R in the Silicon Valley Machine Learning Meetup (&lt;a href=&quot;http://www.meetup.com/Silicon-Valley-Machine-Learning/events/221425830/&quot;&gt;link&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Distributed R parallelizes several R machine learning algorithms across multiple cores and machines in a cluster. So far we have implementations for linear/logistic regression, k-means, Random Forests and Page Rank. If you work with big data in R you should definitely check it out. It’s open source and available on &lt;a href=&quot;https://github.com/vertica/DistributedR&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/CRGgaGllhWzp5Y&quot; width=&quot;425&quot; height=&quot;355&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen=&quot;&quot;&gt; &lt;/iframe&gt;
&lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;strong&gt; &lt;a href=&quot;//www.slideshare.net/JorgeMartinez223/distributed-r-the-next-generation-platform-for-predictive-analytics&quot; title=&quot;Distributed R: The Next Generation Platform for Predictive Analytics&quot; target=&quot;_blank&quot;&gt;Distributed R: The Next Generation Platform for Predictive Analytics&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&quot;//www.slideshare.net/JorgeMartinez223&quot; target=&quot;_blank&quot;&gt;Jorge Martinez de Salinas&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
</description>
        <pubDate>Wed, 15 Apr 2015 00:00:00 -0700</pubDate>
        <link>http://jorgemarsal.github.io/blog/2015/04/15/introducing-distributed-r-in-the-silicon-valley-machine-learning-meetup.html</link>
        <guid isPermaLink="true">http://jorgemarsal.github.io/blog/2015/04/15/introducing-distributed-r-in-the-silicon-valley-machine-learning-meetup.html</guid>
        
        
      </item>
    
      
      <item>
        <title>Tracking memory usage in Linux</title>
        <description>&lt;p&gt;When working with big data optimizing the memory footprint is important.&lt;/p&gt;

&lt;p&gt;In this example we’re serializing a data frame with 50 million elements using R’s native &lt;code&gt;serialize&lt;/code&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;df &amp;lt;- data.frame(runif(50e6,1,10))
ser &amp;lt;- serialize(df,NULL)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each element is a double that takes 8 bytes. If we do the math the data frame should be 400MB (50M elements, 8byte each). The serialized version should be around 400MB too. However if we run that code and check the memory usage we see the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /proc/15155/status |grep Vm
...
VmHWM:	 1207792 kB
VmRSS:	  817272 kB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;VmRSS&lt;/code&gt; is the resident memory and in this case it’s around 800MB as we’d expect. However the peak memory usage (&lt;code&gt;VmHWM&lt;/code&gt;) is 1.2GB. Let’s fire up &lt;code&gt;GDB&lt;/code&gt; and see what’s going on. The relevant R code is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;InitMemOutPStream(&amp;amp;out, &amp;amp;mbs, type, version, hook, fun);
R_Serialize(object, &amp;amp;out);
val =  CloseMemOutPStream(&amp;amp;out);
...
return val;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we set a breakpoint right after &lt;code&gt;R_serialize&lt;/code&gt; we see that the memory usage is around 800MB, as we’d expect:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;VmRSS:	  816664 kB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However is we step into &lt;code&gt;CloseMemOutPStream&lt;/code&gt; we see this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PROTECT(val = allocVector(RAWSXP, mb-&amp;gt;count));
memcpy(RAW(val), mb-&amp;gt;buf, mb-&amp;gt;count);
free_mem_buffer(mb);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code is allocating a whole new buffer and copying the serialized object there. If we set a breakpoint just before &lt;code&gt;free&lt;/code&gt;, the memory usage at that point is 1.2GB.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;VmRSS:	 1207384 kB
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;optimizing-the-code&quot;&gt;Optimizing the code&lt;/h3&gt;

&lt;p&gt;Using R’s &lt;code&gt;serialize&lt;/code&gt; we need 3x the amount of original memory (1.2GB for a 400MB) data frame, which is not acceptable.&lt;/p&gt;

&lt;p&gt;Ideally we’d like to avoid the last copy and just serialize the object in a final buffer.&lt;/p&gt;

&lt;p&gt;The other improvement would be to serialize the object in chunks. E.g. we could have a 10MB buffer and stream parts of the serialized object from that buffer. In this case the peak memory usage would be 410MB instead of 1.2GB!&lt;/p&gt;
</description>
        <pubDate>Mon, 16 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://jorgemarsal.github.io/blog/2015/03/16/tracking-memory-usage-in-linux.html</link>
        <guid isPermaLink="true">http://jorgemarsal.github.io/blog/2015/03/16/tracking-memory-usage-in-linux.html</guid>
        
        
      </item>
    
      <item>
        <title>An interesting bug</title>
        <description>&lt;p&gt;The other day I hit an interesting bug and I thought it would be fun to share it.&lt;/p&gt;

&lt;p&gt;The interesting part is that it only happens with some compiler versions.  E.g. when compiled with GCC 4.8.2 on Ubuntu the software hangs. When compiled with GCC 4.6.4 it doesn’t. Let’s see what’s going on:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(distributedR)
distributedR_start(inst=1, log=3)
Workers registered - 1/1.
All 1 workers are registered.
Master address:port - 127.0.0.1:50002
&amp;lt;hung&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The prompt is not coming back. If we run &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;R-executor-bin&lt;/code&gt; is stuck using 100% of a core.&lt;/p&gt;

&lt;h2 id=&quot;debugging&quot;&gt;Debugging&lt;/h2&gt;

&lt;p&gt;Let’s see where is stuck. First we need the pid of &lt;code&gt;R-executor-bin&lt;/code&gt; which in this case is &lt;code&gt;57725&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gdb bin/R-executor-bin 57725
...
0x0000000000443e55 in dataptr (x=0x6509e60) at  /usr/local/lib/R/site-library/Rcpp/include/Rcpp/routines.h:197
197	inline void* dataptr(SEXP x){
(gdb) bt
#0  0x0000000000443e55 in dataptr (x=0x6509e60) at /usr/local/lib/R/site-library/Rcpp/include/Rcpp/routines.h:197
#1  0x000000000043aee0 in r_vector_start&amp;lt;24&amp;gt; (x=0x6509e60) at /usr/local/lib/R/site-library/Rcpp/include/Rcpp/internal/r_vector.h:32
#2  update (v=&amp;lt;synthetic pointer&amp;gt;, this=&amp;lt;synthetic pointer&amp;gt;) at /usr/local/lib/R/site-library/Rcpp/include/Rcpp/vector/traits.h:40
#3  update (this=&amp;lt;synthetic pointer&amp;gt;) at /usr/local/lib/R/site-library/Rcpp/include/Rcpp/vector/Vector.h:428
#4  set__ (x=&amp;lt;optimized out&amp;gt;, this=&amp;lt;synthetic pointer&amp;gt;) at /usr/local/lib/R/site-library/Rcpp/include/Rcpp/storage/PreserveStorage.h:22
#5  Vector (size=@0x7fffb4f70bc0: 38, this=&amp;lt;synthetic pointer&amp;gt;) at /usr/local/lib/R/site-library/Rcpp/include/Rcpp/vector/Vector.h:107
#6  ReadRawArgs (R=...) at platform/executor/src/executor.cpp:474
#7  0x0000000000436659 in main (argc=&amp;lt;optimized out&amp;gt;, argv=&amp;lt;optimized out&amp;gt;) at platform/executor/src/executor.cpp:709
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The process is stuck in the &lt;code&gt;dataptr&lt;/code&gt; function. GDB tell me is this one:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Rcpp/routines.h:197&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;197	inline void* dataptr(SEXP x){
198	    typedef void* (*Fun)(SEXP) ;
199	    static Fun fun = GET_CALLABLE(&quot;dataptr&quot;) ;
200	    return fun(x) ;
201	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The function is calling itself in an infinite loop! This code doesn’t make any sense. The function above is used to call &lt;code&gt;dataptr&lt;/code&gt; at runtime.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;GET_CALLABLE&lt;/code&gt; is a macro pointing to &lt;code&gt;R_GetCCallable&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#define GET_CALLABLE(__FUN__) (Fun) R_GetCCallable( &quot;Rcpp&quot;, __FUN__ )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s see what’s &lt;code&gt;R_GetCCallable&lt;/code&gt;. It’s defined in &lt;code&gt;Rdynload.h&lt;/code&gt; in R:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/* Experimental interface for exporting and importing functions from
one package for use from C code in a package.  The registration
part probably ought to be integrated with the other registrations.
The naming of these routines may be less than ideal. */

void R_RegisterCCallable(const char *package, const char *name, DL_FUNC fptr);
DL_FUNC R_GetCCallable(const char *package, const char *name);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The corresponding registration part is in &lt;code&gt;Rcpp_init.cpp:109&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RCPP_REGISTER(dataptr)
#define RCPP_REGISTER(__FUN__) R_RegisterCCallable( &quot;Rcpp&quot;, #__FUN__ , (DL_FUNC)__FUN__ );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But wait. That registration is not pointing to &lt;code&gt;dataptr&lt;/code&gt; in &lt;code&gt;Rcpp/routines.h:197&lt;/code&gt;. It’s pointing to another &lt;code&gt;dataptr&lt;/code&gt; in &lt;code&gt;Rcpp/src/barrier.cpp:71&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// [[Rcpp::register]]
void* dataptr(SEXP x){
    return DATAPTR(x);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That’s bad. There are 2 functions with the same name and we’re registering one but getting the other one at runtime! And to add insult to the injury the latter is calling itself in an infinite loop. But there are 2 weird things here:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;This only fails in some compiler versions!&lt;/li&gt;
  &lt;li&gt;The process is hung but is should segfault. If we keep recursing indefinitely at some time we’ll run out of stack space and the process will be killed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s try to find out the reason for #1. Let’s compile with both GCC 4.6.4 and GCC 4.8.2 and see the differences in the symbol tables:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nm --dynamic R-executor-bin-4.8 |grep dataptr
0000000000443e50 W _Z7dataptrP7SEXPREC
00000000006c6060 u _ZGVZ7dataptrP7SEXPRECE3fun
00000000006c5fe8 u _ZZ7dataptrP7SEXPRECE3fun

$ nm --dynamic  R-executor-bin-4.6 |grep dataptr
00000000006cafa0 u _ZGVZ7dataptrP7SEXPRECE3fun
00000000006cafa8 u _ZZ7dataptrP7SEXPRECE3fun
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Aha! GCC 4.8 has removed one of the &lt;code&gt;dataptr&lt;/code&gt; definitions (the wrong one) so we’re getting what we expect at runtime. Unfortunately GCC 4.6 has not.&lt;/p&gt;

&lt;h2 id=&quot;reproducing-the-hang-with-a-small-program&quot;&gt;Reproducing the hang with a small program&lt;/h2&gt;

&lt;p&gt;Let’s try to reproduce it with a small example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;RInside.h&amp;gt;
#include &amp;lt;iostream&amp;gt;

int main() {
    RInside R(0, 0);
    int size = 38;
    Rcpp::RawVector raw(size);
    std::cout &amp;lt;&amp;lt; &quot;done&quot; &amp;lt;&amp;lt; std::endl;
    return 0;
}

$ g++-4.6 -I/usr/local/lib/R/site-library/RInside/include/ -I/usr/local/lib/R/site-library/Rcpp/include/ -I/usr/share/R/include/ -O0 -g  -o rcpphang-4.6 rcpphang.cpp -lR -lRInside -L/usr/local/lib/R/site-library/RInside/lib/
$ g++-4.8 -I/usr/local/lib/R/site-library/RInside/include/ -I/usr/local/lib/R/site-library/Rcpp/include/ -I/usr/share/R/include/ -O0 -g  -o rcpphang-4.8 rcpphang.cpp -lR -lRInside -L/usr/local/lib/R/site-library/RInside/lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s run it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./rcpphang-4.8
done
$ ./rcpphang-4.6
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait, what? This is working! No hang. Let’s take a look at the symbol tables:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nm --dynamic rcpphang-4.6 |grep dataptr
&amp;lt;no output&amp;gt;

$ nm --dynamic rcpphang-4.8 |grep dataptr
&amp;lt;no output&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, that’s why it’s working. &lt;code&gt;dataptr&lt;/code&gt; is not in the dynamic symbol tables. Looking at the original SW (that hung) I see we compile with &lt;code&gt;-rdynamic&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;-rdynamic&lt;/strong&gt;: Pass the flag -export-dynamic to the ELF linker, on targets that support it. This instructs the linker to add all symbols, not only used ones, to the dynamic symbol table. This option is needed for some uses of dlopen or to allow obtaining backtraces from within a program.&lt;/p&gt;

&lt;p&gt;Let’s compile with &lt;code&gt;-rdynamic&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ g++-4.6 -I/usr/local/lib/R/site-library/RInside/include/ -I/usr/local/lib/R/site-library/Rcpp/include/ -I/usr/share/R/include/ -O0 -g -rdynamic -o rcpphang-4.6 rcpphang.cpp -lR -lRInside -L/usr/local/lib/R/site-library/RInside/lib/
$ g++-4.8 -I/usr/local/lib/R/site-library/RInside/include/ -I/usr/local/lib/R/site-library/Rcpp/include/ -I/usr/share/R/include/ -O0 -g  -o rcpphang-4.8 rcpphang.cpp -lR -lRInside -L/usr/local/lib/R/site-library/RInside/lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The symbols are there now:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nm --dynamic rcpphang-4.6-rdyn-noopt|grep dataptr
00000000004039f8 W _Z7dataptrP7SEXPREC
0000000000606668 u _ZGVZ7dataptrP7SEXPRECE3fun
0000000000606670 u _ZZ7dataptrP7SEXPRECE3fun

$ nm --dynamic rcpphang-4.8-rdyn-noopt|grep dataptr
0000000000403a08 W _Z7dataptrP7SEXPREC
0000000000606648 u _ZGVZ7dataptrP7SEXPRECE3fun
0000000000606650 u _ZZ7dataptrP7SEXPRECE3fun

$ ./rcpphang-4.8
&amp;lt;segfault&amp;gt;

$ ./rcpphang-4.6
&amp;lt;segfault&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, now both have the symbol and therefore both segfault! The other difference is that we’re compiling the original SW with &lt;code&gt;-O2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ g++-4.6 -I/usr/local/lib/R/site-library/RInside/include/ -I/usr/local/lib/R/site-library/Rcpp/include/ -I/usr/share/R/include/ -O2 -g -rdynamic -o rcpphang-4.6 rcpphang.cpp -lR -lRInside -L/usr/local/lib/R/site-library/RInside/lib/
$ g++-4.8 -I/usr/local/lib/R/site-library/RInside/include/ -I/usr/local/lib/R/site-library/Rcpp/include/ -I/usr/share/R/include/ -O2 -g  -o rcpphang-4.8 rcpphang.cpp -lR -lRInside -L/usr/local/lib/R/site-library/RInside/lib/


$ nm --dynamic rcpphang-4.8|grep dataptr
00000000006046a8 u _ZGVZ7dataptrP7SEXPRECE3fun
00000000006046a0 u _ZZ7dataptrP7SEXPRECE3fun
$ nm --dynamic rcpphang-4.6|grep dataptr
0000000000402b50 W _Z7dataptrP7SEXPREC
00000000006046b8 u _ZGVZ7dataptrP7SEXPRECE3fun
00000000006046c0 u _ZZ7dataptrP7SEXPRECE3fun
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have the same behavior as with the original SW! GCC 4.8 has removed the symbol while 4.6 hasn’t. That’s why 4.8 works and 4.6 hangs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./rcpphang-4.8
done

$ ./rcpphang-4.6
&amp;lt;hung&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now regarding weirdness #2, the explanation for the hang instead of the segfault is probably that the compiler is doing &lt;a href=&quot;http://c2.com/cgi/wiki?TailCallOptimization&quot;&gt;tail call optimization&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;fixing-the-problem&quot;&gt;Fixing the problem&lt;/h2&gt;

&lt;p&gt;One way to fix this issue is to make sure the conflicting function is not exported in the shared library. We can achieve that adding the following attribute:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;inline __attribute__ ((visibility (&quot;hidden&quot;))) void* dataptr(SEXP x){
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end we reported the bug to the &lt;code&gt;Rcpp&lt;/code&gt; author and he applied the fix in &lt;code&gt;Rcpp 0.11.5&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;That was fun :)&lt;/p&gt;
</description>
        <pubDate>Mon, 16 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://jorgemarsal.github.io/blog/2015/03/16/an-interesting-bug.html</link>
        <guid isPermaLink="true">http://jorgemarsal.github.io/blog/2015/03/16/an-interesting-bug.html</guid>
        
        
      </item>
    
  </channel>
</rss>
