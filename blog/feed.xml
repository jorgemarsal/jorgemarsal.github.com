<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jorge Martinez</title>
    <description></description>
    <link>http://jorgemarsal.github.io/blog/</link>
    <atom:link href="http://jorgemarsal.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 07 Jan 2016 03:32:51 -0800</pubDate>
    <lastBuildDate>Thu, 07 Jan 2016 03:32:51 -0800</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>100000-foot view of a modern web service </title>
        <description>&lt;p&gt;In this post we’re going to explore how to deliver a modern web service. To make things clearer we’re going to use a OCR service as an example. All the code is available on &lt;a href=&quot;https://github.com/jorgemarsal/webocr/tree/master/webocr&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;WebOCR&lt;/code&gt; extracts text from user uploaded images using a technique known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Optical_character_recognition&quot;&gt;OCR&lt;/a&gt;. Users provide the image information (for now just the URL) and the service downloads the image, extracts the text and sends the result back to the user.&lt;/p&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;
&lt;p&gt;The architecture is shown in this picture below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/webocr/arch.png&quot; alt=&quot;Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are 2 main elements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The WebOCR service provides a restful API for extracting text from images.&lt;/li&gt;
  &lt;li&gt;The task server and associated workers perform the actual OCR processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The basic workflow is:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Users post a request with the image information.
    &lt;ul&gt;
      &lt;li&gt;The WebOCR service sends the task to the task server and …&lt;/li&gt;
      &lt;li&gt;… replies with a 202 Accepted code and a link to check the task progress.&lt;/li&gt;
      &lt;li&gt;The task server sends the task to a queue (RabbitMQ in this case).&lt;/li&gt;
      &lt;li&gt;The worker picks up the task from the queue.&lt;/li&gt;
      &lt;li&gt;The server polls the task server periodically to update the task status and stores that information.&lt;/li&gt;
      &lt;li&gt;The client can also access this information through the &lt;code&gt;/queue&lt;/code&gt; endpoint in the WebOCR service.&lt;/li&gt;
      &lt;li&gt;Eventually the task will succeed or fail. If the task succeeds the client will get redirected to the result. If it fails the client can retrieve the failure cause from the /queue endpoint.&lt;/li&gt;
      &lt;li&gt;Both the client and the web server have timeouts and consider the task failed if they don’t receive a response in a timely manner.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is an example exchange:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -H &quot;Content-Type: application/json&quot; -X POST \
-d '{&quot;url&quot;:&quot;http://localhost:1234/static/ocr/sample1.jpg&quot;}' \
http://localhost:1234/api/v1/services
...
HTTP/1.1 202 Accepted
Content-Location: /queue/7bc6c484-df47-4cb6-b254-17b770b52060
….
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The server replies with a 202 code and a URL to query the task status. If we access that URL and the task is still pending we’ll get a 200 ok and info about the task. When the task is done we’ll get a 303 and a link to the newly created resource:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v http://localhost:1234/queue/abb5fbbf-9519-4d21-af23-e3a7da1ca480

….
HTTP/1.1 303 See Other
Location: /api/v1/service/90
…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s get the result:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -v http://localhost:1234/api/v1/service/90
HTTP/1.1 200 OK
{
    &quot;result&quot;: &quot;THINGS YOU SHOULD KNOW ABOU ...
              ... neiraviers common In\n\n&quot;,
    &quot;state&quot;: &quot;SUCCESS&quot;,
    &quot;url&quot;: &quot;http://localhost:1234/static/ocr/sample1.jpg&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We also have a websocket channel to send real time information of the task progress to the frontend.&lt;/p&gt;

&lt;p&gt;The next sections go into more detail on how all of this is implemented.&lt;/p&gt;

&lt;h3 id=&quot;backend&quot;&gt;Backend&lt;/h3&gt;
&lt;p&gt;The backend is a Tornado app that implements the restful API. There aren’t any blocking operations. We either use event-loop aware functionality (like Tornado’s &lt;code&gt;AsyncHTTPClient&lt;/code&gt; or run the task in a different thread with the &lt;code&gt;@run_on_executor&lt;/code&gt; decorator. By using this design the server can handle many concurrent user connections.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@run_on_executor
def _on_executor(self, fn, *args, **kwargs):
    &quot;&quot;&quot;Execute the given function on another thread&quot;&quot;&quot;
    return fn(*args, **kwargs)

@gen.coroutine
def f():
    http_client = AsyncHTTPClient()
    response = yield http_client.fetch(&quot;http://example.com&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;database&quot;&gt;Database&lt;/h4&gt;

&lt;p&gt;Choosing the right database is probably one of the most important decision in a web project. In this case, since the performance requirements are modest, we don’t need a database cluster and we can live with a single node and snapshot backups. In this situation we can pretty much do whatever we want and we’ve chosen MySQL. When the performance requirements are higher then we need to consider other solutions and keep in mind the associated performance and consistency tradeoffs.&lt;/p&gt;

&lt;p&gt;As mentioned earlier the backend stores the state in a MySQL database but instead of having a fixed schema we have a &lt;code&gt;schemaless&lt;/code&gt; design. The original idea comes from &lt;a href=&quot;https://backchannel.org/blog/friendfeed-schemaless-mysql&quot;&gt;Friendfeed’s schemaless design&lt;/a&gt; and the implementation (with minor modifications) comes from &lt;a href=&quot;https://github.com/eklitzke/schemaless&quot;&gt;Evan Klitzke&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this design we store compressed pickled python dictionaries that can hold arbitrary data, making schema evolution simple. Note that this idea is from 2009 when NoSQL offerings weren’t as mature as today. Probably nowadays we’d be better off using something like Mongo. Storing and querying the database is done like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;row = self.db.put(dict(url=service['url'], state='PENDING'))
rows = url_index.query(c.url == url)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;task-server&quot;&gt;Task server&lt;/h4&gt;
&lt;p&gt;We’ve decided to offload the actual computations to a task server. This way we can keep a simple WebOCR service and scale the task server independently if the load increases.&lt;/p&gt;

&lt;p&gt;The task server exposes another REST API to send tasks to Celery. Celery is an asynchronous task queue/job queue based on distributed message passing. The workflow is quite simple. The WebOCR service posts tasks to the task server. The task server sends those tasks to a queue (RabbitMQ in this case) and on the other end, one or more task workers pick up the tasks.&lt;/p&gt;

&lt;h4 id=&quot;advice-for-developing-backend-code&quot;&gt;Advice for developing backend code&lt;/h4&gt;

&lt;p&gt;Giving a complete overview of all the things to keep in mind when developing backend SW is out of scope for this post. However we’ll just give a few pieces of advice.&lt;/p&gt;

&lt;p&gt;We decided to develop the service in Python because it’s a language we’re pretty familiar with. Developing in python is really fast due to its dynamic nature and it comes with libraries for almost everything. Node.js could have been a good option as well. If type safety or performance are important considerations then Go would have been a better alternative.&lt;/p&gt;

&lt;p&gt;A lot of advice about Python itself can be found in the fantastic book &lt;a href=&quot;http://www.effectivepython.com/&quot;&gt;Effective Python&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One of the most important things to do when developing software is having good testing. One rule that I find particularly useful to increase confidence is having a high test coverage. Keep in mind that high coverage is not enough though. It’s important to design a very comprehensive test plan. Otherwise you can have 100% coverage but ignore a lot of real-world cases. A tool that works well is &lt;code&gt;pytest-cov&lt;/code&gt;. You can get coverage numbers using this command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;py.test --cov=webocr --cov-report=html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As an example if we want to test a call to a remote service we should at least test a couple scenarios:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Service works ok&lt;/li&gt;
  &lt;li&gt;Service not running&lt;/li&gt;
  &lt;li&gt;Service returns HTTP errrors&lt;/li&gt;
  &lt;li&gt;Service returns 200 ok but response data is malformed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To easily test all those error conditions we can mock the service using somethind like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class App(object):
    def __init__(self, service):
        self.service = service

    def do(self):
        try:
            self.service.do(something)
        except ServiceError as e:
            handle_exception()

def test_normal():
    “”” Normal service “””
    app = App(service)

def test_error():
    “”” Mock service that triggers all the error conditions “””
    app = App(mock_service)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Correctness is absolute necessary but you should also take a look at the performance of the service. You can use something like &lt;a href=&quot;http://jmeter.apache.org&quot;&gt;Jmeter&lt;/a&gt; for that.&lt;/p&gt;

&lt;p&gt;Finally a linting tool like &lt;code&gt;flakes8&lt;/code&gt; can point common errors and style issues.&lt;/p&gt;

&lt;p&gt;I recommend having testing, linting and other tooling as part of the CI process both on the client (via Git hooks) and on the server (with something like Jenkins).&lt;/p&gt;

&lt;h3 id=&quot;frontend&quot;&gt;Frontend&lt;/h3&gt;

&lt;p&gt;We’ve created a simple webpage to submit new tasks and to display the status of pending/finished tasks (see pic below). On the left hand side we have a form to submit the image information. In the middle we display the original image and on the right the extracted text. We also have a websocket channel to receive real-time progress updates.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/webocr/frontend.png&quot; alt=&quot;Frontend&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m by no means a frontend expert. I just know the minimum HTML, CSS and JS to get by. Having said that, it pays off to organize the code properly. For JS we follow the MVC pattern. The important point to remember is that the model and the view don’t talk directly, they always go through the controller.&lt;/p&gt;

&lt;h3 id=&quot;infrastructure&quot;&gt;Infrastructure&lt;/h3&gt;

&lt;p&gt;Developing the software is just half the problem Then you have to reliably run it in production.
In this section I’m gonna talk about 2 great pieces of software that we use and help greatly: Docker and Kubernetes.&lt;/p&gt;

&lt;p&gt;Docker gives you a single environment in which you have total control over dependencies and versions. And the best part is that the environment will be the same (or very similar) in development and production. We recommend dockerizing every service.&lt;/p&gt;

&lt;p&gt;The other piece of the puzzle is kubernetes. Kubernetes is an open source orchestration system for Docker containers that provides fault tolerance and high availability.
One of the main abstraction in Kubernetes is the Replication Controller. You can specify how many pods (collection of containers) you want to have running and kubernetes will enforce that. Kubernetes performs health checks and automatically restarts the pods if there are errors. Another interesting feature is the ability to increase the number of pods based on load. That way you can scale your service up or down based on load. In this example we can increase the number of task workers if the load is high and reduce it to save costs if the load decreases.&lt;/p&gt;

&lt;p&gt;The other important abstraction is the service. Services provide discoverability and have neat features like automatic load balancing of the traffic between pods. Kubernetes also comes with solutions for logging and monitoring.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;With this we conclude this blog post. We hope the ideas presented here will help you with your own designs. The complete code is up on &lt;a href=&quot;https://github.com/jorgemarsal/webocr/tree/master/webocr&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Jan 2016 00:00:00 -0800</pubDate>
        <link>http://jorgemarsal.github.io/blog/2016/01/06/100000-foot-view-of-a-modern-web-service.html</link>
        <guid isPermaLink="true">http://jorgemarsal.github.io/blog/2016/01/06/100000-foot-view-of-a-modern-web-service.html</guid>
        
        
      </item>
    
      <item>
        <title>Creating your first service using Kubernetes on AWS</title>
        <description>&lt;p&gt;Lately we’ve playing with Kubernetes (or K8s) at work. Kubernetes is an open source orchestration system for Docker containers. It comes with awesome features like fault tolerance, auto-scaling and others.&lt;/p&gt;

&lt;p&gt;In this post we give an overview on how to install Kubernetes on AWS and how to run your first service.&lt;/p&gt;

&lt;h3 id=&quot;installing-kubernetes-on-aws&quot;&gt;Installing Kubernetes on AWS&lt;/h3&gt;

&lt;p&gt;The first step is to create a new instance. Once the instance has booted up, log in and enter your AWS credentials (the K8s installer needs the credentials to create objects in AWS):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p ~/.aws/
$ vi ~/.aws/credentials
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s install Kubernetes next. There are a few of &lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/master/cluster/aws/options.md&quot;&gt;options&lt;/a&gt; that can be tweaked:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export KUBERNETES_PROVIDER=aws
$ export MASTER_SIZE=t2.large
$ export NODE_SIZE=t2.large
$ wget -q -O - https://get.K8s.io | bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After this command finishes K8s is up and running! By default the scripts creates a cluster with 1 master node and 4 minion nodes. It also creates 2 security groups that allow SSH access to master and minions and HTTP access to the master.&lt;/p&gt;

&lt;p&gt;K8s provides a REST API to interface with the cluster. The information to access that API including the URL, username, password and tokens can be found in the config file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /home/ec2-user/.kube/config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In our case the API lives in the following URL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://52.35.142.134

{
 &quot;paths&quot;: [
   &quot;/api&quot;,
   &quot;/api/v1&quot;,
   ...
   &quot;/ui/&quot;,
   &quot;/version&quot;
 ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead of using the REST API directly we’re going to use a CLI tool called &lt;code&gt;kubelet&lt;/code&gt;. Let’s add the paths and export a few environment variables in ~/.bashrc:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export PATH=$PATH:/home/ec2-user/kubernetes/cluster/:$HOME/kubernetes/platforms/linux/amd64/
$ export KUBERNETES_PROVIDER=aws
$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;K8s runs a few services by default including DNS, logging, monitoring and others:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl cluster-info
Kubernetes master is running at https://52.35.87.147
Elasticsearch is running at https://52.35.87.147/api/v1/proxy/namespaces/kube-system/services/elasticsearch-logging
Heapster is running at https://52.35.87.147/api/v1/proxy/namespaces/kube-system/services/heapster
Kibana is running at https://52.35.87.147/api/v1/proxy/namespaces/kube-system/services/kibana-logging
KubeDNS is running at https://52.35.87.147/api/v1/proxy/namespaces/kube-system/services/kube-dns
KubeUI is running at https://52.35.87.147/api/v1/proxy/namespaces/kube-system/services/kube-ui
Grafana is running at https://52.35.87.147/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
InfluxDB is running at https://52.35.87.147/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;logging&quot;&gt;Logging&lt;/h4&gt;
&lt;p&gt;Kubernetes’ default option for logging is to run a &lt;code&gt;fluentd&lt;/code&gt; agent on each node. These agents send the logs to &lt;code&gt;ElasticSearch&lt;/code&gt; where they’re stored and indexed. To explore the logs there’s a GUI named &lt;code&gt;Kibana&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/k8s-aws/kibana.jpg&quot; alt=&quot;Logging&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;monitoring&quot;&gt;Monitoring&lt;/h4&gt;
&lt;p&gt;There’s an analogous solution for monitoring as well. Each node runs a &lt;code&gt;heapster&lt;/code&gt; agent that collects metrics. The metrics are sent to &lt;code&gt;influxdb&lt;/code&gt; (a time series database) where they can be queried using a REST API or a GUI named &lt;code&gt;Grafana&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/k8s-aws/grafana.jpg&quot; alt=&quot;Monitoring&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;kube-ui&quot;&gt;Kube-ui&lt;/h3&gt;
&lt;p&gt;Kubernetes comes with a graphical dashboard with information of the nodes that form the cluster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/k8s-aws/kube-ui.jpg&quot; alt=&quot;Kube-UI&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And of course there’s the possibility to SSH to the nodes directly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh -i ~/.ssh/kube_aws_rsa ubuntu@52.35.178.251
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;running-your-own-service-on-k8s&quot;&gt;Running your own service on K8s&lt;/h3&gt;
&lt;p&gt;To run your service on K8s the first step is to create a Docker image out of it.
In this example we’re going to Dockerize the &lt;code&gt;hello-node&lt;/code&gt; service. This is just a simple web server that outputs “Hello world” when accessed. To create a Docker image we just add a simple Dockerfile to the project:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM node
ADD index.js
CMD node index.js
EXPOSE 8000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then let’s make sure Docker is installed and running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo yum -y install docker git
$ sudo service docker start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s get the code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone http://github.com/jorgemarsal/hello-node
$ cd hello-node
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To build the image we run &lt;code&gt;docker build&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker build -t jorgemarsal/hello-node:0.0.1 .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we can push the image to a repository. Later on Kubernetes will fetch the image from there:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker push jorgemarsal/hello-node:0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;kubernetes-spec-files&quot;&gt;Kubernetes spec files&lt;/h4&gt;

&lt;p&gt;To run the service on K8s we need to create a &lt;code&gt;Replication Controller&lt;/code&gt; spec and a &lt;code&gt;Service&lt;/code&gt; spec. In the replication controller spec we specify the docker image, the number of replicas and the port the application is listening on:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// rc.json
{
   &quot;apiVersion&quot;: &quot;v1&quot;,
   &quot;kind&quot;: &quot;ReplicationController&quot;,
   &quot;metadata&quot;: {
       &quot;labels&quot;: {
           &quot;app&quot;: &quot;hello-node&quot;,
           &quot;version&quot;: &quot;v1&quot;
       },
       &quot;name&quot;: &quot;hello-node-v1&quot;,
       &quot;namespace&quot;: &quot;&quot;
   },
   &quot;spec&quot;: {
       &quot;replicas&quot;: 1,
       &quot;selector&quot;: {
           &quot;app&quot;: &quot;hello-node&quot;,
           &quot;version&quot;: &quot;v1&quot;
       },
       &quot;template&quot;: {
           &quot;metadata&quot;: {
               &quot;labels&quot;: {
                   &quot;app&quot;: &quot;hello-node&quot;,
                   &quot;version&quot;: &quot;v1&quot;
               }
           },
           &quot;spec&quot;: {
               &quot;containers&quot;: [
                   {
                       &quot;image&quot;: &quot;jorgemarsal/hello-node:0.0.1&quot;,
                       &quot;name&quot;: &quot;hello-node-v1&quot;,
                       &quot;ports&quot;: [
                           {
                               &quot;containerPort&quot;: 8000,
                               &quot;name&quot;: &quot;hello-node&quot;
                           }
                       ]
                   }
               ]
           }
       }
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s create the Replication Controller:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f rc.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the service spec we specify the ports and we request a load balancer for our service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// service.json
{
  &quot;kind&quot;:&quot;Service&quot;,
  &quot;apiVersion&quot;:&quot;v1&quot;,
  &quot;metadata&quot;:{
     &quot;name&quot;:&quot;hello-node&quot;,
     &quot;labels&quot;:{
        &quot;app&quot;:&quot;hello-node&quot;,
        &quot;version&quot;:&quot;v1&quot;
     }
  },
  &quot;spec&quot;:{
     &quot;ports&quot;: [
         {
             &quot;name&quot;: &quot;hello-node&quot;,
             &quot;port&quot;: 8000,
             &quot;targetPort&quot;: 8000
         }
     ],
     &quot;selector&quot;:{
        &quot;app&quot;:&quot;hello-node&quot;,
        &quot;version&quot;:&quot;v1&quot;
     },
     &quot;type&quot;: &quot;LoadBalancer&quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s create the service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f svc.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few seconds the service should be up and running. We can check with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods
$ kubectl get services
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since we specified &lt;code&gt;type: loadbalancer&lt;/code&gt;, AWS automatically creates a ELB for us.&lt;/p&gt;

&lt;p&gt;We can access the service using the ELB URL. In a real-world example probably we’d want a friendlier DNS name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl http://a9abab161a37511e5810906731b2c52d-1058840568.us-west-2.elb.amazonaws.com:8000/
hello world
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Even though the &lt;code&gt;hello-node&lt;/code&gt; service is extremely simple, we get a few nice things for free. Kubernetes is constantly polling the service and if something goes wrong the service is automatically restarted on another node. We also have the possibility to scale the number of hello-node containers up or down easily. Finally we get logging and monitoring.&lt;/p&gt;

&lt;h3 id=&quot;acessing-the-k8s-api-programmatically&quot;&gt;Acessing the K8s API programmatically&lt;/h3&gt;
&lt;p&gt;Doing things manually is fine to start getting familiar with the system, but for more serious projects you should access the K8s API programmatically. At work we mainly use Python and we’ve found a simple library named &lt;a href=&quot;https://github.com/eldarion-gondor/pykube&quot;&gt;pykube&lt;/a&gt; that’s pretty useful.&lt;/p&gt;

&lt;p&gt;Pykube needs a few parameters to function. As mentioned previously we can get those parameters from Kubernetes’ configuration file.
It’s also possible to access the API from within a running container by using the following parameters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;URL: https://kubernetes&lt;/li&gt;
  &lt;li&gt;user = nobody&lt;/li&gt;
  &lt;li&gt;token present on file &lt;code&gt;/var/run/secrets/kubernetes.io/serviceaccount/token&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;CA present on file: &lt;code&gt;/var/run/secrets/kubernetes.io/serviceaccount/ca.crt&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this &lt;a href=&quot;https://gist.github.com/jorgemarsal/600c49a12d79429c7da1&quot;&gt;gist&lt;/a&gt; you can find some functions to help you get started.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kubernetes.io&quot;&gt;Kubernetes official website&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://docker.com&quot;&gt;Docker official website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hope this helps you get started with your next kubernetes project on AWS!&lt;/p&gt;
</description>
        <pubDate>Tue, 15 Dec 2015 00:00:00 -0800</pubDate>
        <link>http://jorgemarsal.github.io/blog/2015/12/15/creating-your-first-service-using-kubernetes-on-aws.html</link>
        <guid isPermaLink="true">http://jorgemarsal.github.io/blog/2015/12/15/creating-your-first-service-using-kubernetes-on-aws.html</guid>
        
        
      </item>
    
      <item>
        <title>Introducing hdfsconnector for R</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Today we’re excited to introduce a new &lt;a href=&quot;https://github.com/vertica/r-dataconnector&quot;&gt;connector&lt;/a&gt; for Distributed R that enables R users to read CSV and ORC files from HDFS. One of its main features is the ability to parallelize file loads across multiple machines, drastically improving performance. This post gives an overview of how the connector was designed.&lt;/p&gt;

&lt;h3 id=&quot;parallelizing-file-load&quot;&gt;Parallelizing file load&lt;/h3&gt;
&lt;p&gt;To reduce the time it takes to load a big file the connector splits the loading across different machines in the cluster.&lt;/p&gt;

&lt;p&gt;To understand how this works let’s walk through an example. Say we have a 4GB CSV file (test4GB.csv) and a single machine with 4 executors (4 cores). The connector creates as many partitions as executors. In this case each core will load 1GB of the CSV file. When working with ORC files we don’t have that much flexibility because the level of parallelism is already defined (parallelism == number of stripes in the file). In the ORC case we’ll assign different stripes to different executors.&lt;/p&gt;

&lt;p&gt;Once we’ve split the file into multiple chunks we assign each chunk to the best executor. For files stored in HDFS we try to minimize data movement in the cluster. This means we’ll assign the chunk to the executor that has the highest number of HDFS blocks locally. In this case executor 1 has the highest number of blocks corresponding to chunk 1 (red), executor 2 has the highest number of blocks for chunk 2 and so on.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/hdfsconnector/scheduling.png&quot; alt=&quot;Scheduling&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After creating the scheduling map the master sends a message to the executors indicating which chunks should be loaded on which executor. In this case the master sends 1 message to each executor:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To executor 1: load test4GB.csv, range: [0,   1GB), schema …&lt;/li&gt;
  &lt;li&gt;To executor 2: load test4GB.csv, range: [1GB, 2GB), schema …&lt;/li&gt;
  &lt;li&gt;To executor 3: load test4GB.csv, range: [2GB, 3GB), schema …&lt;/li&gt;
  &lt;li&gt;To executor 4: load test4GB.csv, range: [3GB, 4GB), schema …&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;loading-individual-chunks-on-the-executors&quot;&gt;Loading individual chunks on the executors&lt;/h3&gt;

&lt;p&gt;Each executor implements a loading stack made of 4 layers (Assembler, Record Parser, Split Producer and Block Reader). To understand what each layer does it’s useful to start from the bottom. The file is just an array of bytes. We extract splits from the byte array (e.g. lines in CSV). Then we extract individual records from that split (fields in that line). Finally we assemble a final object out of individual records (e.g. an R dataframe).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/hdfsconnector/loading_stack.png&quot; alt=&quot;Loading stack&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the next paragraphs we give more details about how the different layers work:&lt;/p&gt;

&lt;h4 id=&quot;block-reader&quot;&gt;Block reader&lt;/h4&gt;
&lt;p&gt;Reads a stream of bytes from storage. We have 2 flavors (&lt;code&gt;LocalBlockReader&lt;/code&gt; and &lt;code&gt;HdfsBlockReader&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;LocalBlockReader&lt;/code&gt; reads from the local FS while &lt;code&gt;HdfsBlockReader&lt;/code&gt; reads from HDFS. We’re using the WebHDFS interface but we may decide to switch to the native interface in the future if HTTP is not fast enough.&lt;/p&gt;

&lt;p&gt;To get the best performance we have an I/O thread that pre-fetches blocks while another thread parses the previous block. The idea is to overlap I/O with compute for improved performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/hdfsconnector/pipeline.png&quot; alt=&quot;Pipeline&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;split-producer&quot;&gt;Split producer&lt;/h4&gt;
&lt;p&gt;Assembles splits (lines in CSV), stripes in ORC, etc. For that it might need to read one or more blocks.
We have 2 flavors &lt;code&gt;DelimiterSplitProducer&lt;/code&gt; and &lt;code&gt;OffsetSplitProducer&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In CSV lines are usually determined by a newline character (\n). In this case we use the &lt;code&gt;DelimiterSplitProducer&lt;/code&gt; to extract the lines. This is quite slow as we have to look at every byte to detect newline characters. Other formats (such as ORC) include the split boundaries in the metadata and we can extract the splits directly using the &lt;code&gt;OffsetSplitProducer&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;An additional complication for CSV files is that the offset from which we start reading may not correspond to the beginning of the line (actually this is the normal case). To handle this we have to backtrack one character and then skip that incomplete line. The discarded line will be processed by the executor that’s working on the previous chunk.&lt;/p&gt;

&lt;p&gt;To understand this better let’s use an example. Suppose executor 1 is assigned range [0, 6] and executor 2  range [6,12]. Lines correspond to ranges [0,4),[4,8) and [8,12). Executor 1 will process both lines 1 and 2. Even though the range finishes at offset 6 it will continue until the full 2nd line is consumed. Executor 2 in turn will discard the incomplete line and only process line 3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/hdfsconnector/csv_lines.png&quot; alt=&quot;CSV line splitting&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;record-parser&quot;&gt;Record parser&lt;/h4&gt;
&lt;p&gt;Breaks a split into individual records.
The record parser is format dependent. In the CSV case we use a separator (usually a comma) to separate the records.
ORC is a columnar format so all the records in the same column are stored together in a highly optimized way (encoded and compressed).&lt;/p&gt;

&lt;p&gt;To understand the importance of the format let’s see how an int column is encoded both in CSV and ORC.
Let’s say the data is 1000,1001,1002 …
In CSV each record takes 4 bytes or 32 bits (let’s assume the numbers fit between 0 and 9999). In ORC we’d only need 14 bits to encode each number.
On top of that we can add smart encodings. For example in this case instead of storing the numbers we can store the delta between each consecutive number. That would be [1,1,1,1 …]. This can be run-length encoded to [num-elements,1]. Finally we can apply compression to reduce the final size even more.
ORC supports many different encodings (e.g. dictionary encoding for string type) and different compression types.&lt;/p&gt;

&lt;p&gt;Another advantage of ORC vs CSV is that it supports complex types (struct, map, list …)&lt;/p&gt;

&lt;h4 id=&quot;assembler&quot;&gt;Assembler&lt;/h4&gt;
&lt;p&gt;The final layer assembles a number of records into a final object (e.g. an R dataframe).&lt;/p&gt;

&lt;p&gt;The mapping in CSV is straightforward. In the ORC case is more complex as it supports more types and they can be arbitrarily nested. The mapping we use is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ORC Struct -&amp;gt; R dataframe&lt;/li&gt;
  &lt;li&gt;ORC List -&amp;gt; R list&lt;/li&gt;
  &lt;li&gt;ORC Map -&amp;gt; R dataframe with 2 columns (key, value).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;
&lt;p&gt;Performance is highly dependent on the HW configurations (numbers or cores, performance per core, bisection bandwidth between the HDFS nodes and the nodes loading the files, etc.).&lt;/p&gt;

&lt;p&gt;In the graph we present the results of loading a 64MB CSV file with a single machine (4 cores). The time decreases linearly as we add cores.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/hdfsconnector/perf.png&quot; alt=&quot;Performance&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This new connector allows R users to read files from HDFS. It can parallelize file loads across cores and machines which results in great performance.&lt;/p&gt;

&lt;p&gt;We offer this functionality as part of &lt;a href=&quot;https://github.com/vertica/distributedr&quot;&gt;Distributed R&lt;/a&gt; (supporting distributed operation) and also as an individual package named &lt;a href=&quot;https://github.com/vertica/r-dataconnector&quot;&gt;dataconnector&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The connector is open-source and we encourage feedback and contributions.&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Oct 2015 00:00:00 -0700</pubDate>
        <link>http://jorgemarsal.github.io/blog/2015/10/24/introducing-hdfsconnector-for-r.html</link>
        <guid isPermaLink="true">http://jorgemarsal.github.io/blog/2015/10/24/introducing-hdfsconnector-for-r.html</guid>
        
        
      </item>
    
      <item>
        <title>Design of a simple distributed application</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Today I’m going to explain how to design a simple distributed application. The goal is to read a really big file into memory. Since the file doesn’t fit in a single machine’s memory we need to design a system to split the file across different machines. A possible architecture comprises a &lt;code&gt;master&lt;/code&gt; that breaks the file into smaller chunks and &lt;code&gt;N workers&lt;/code&gt; that process those chunks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/usecase.png&quot; alt=&quot;Use case&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The master should implement the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A way to split the work across workers. Different policies like round-robin or sending to the worker with less requests in flight are possible.&lt;/li&gt;
  &lt;li&gt;A way of registering workers.&lt;/li&gt;
  &lt;li&gt;A way to monitor the workers, notice if they go down and send their pending splits to other workers.&lt;/li&gt;
  &lt;li&gt;A way to timeout if there are no workers registered or the operation takes too long.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Master and workers communicate using the following &lt;code&gt;protobuf&lt;/code&gt; messages over a &lt;code&gt;zeromq&lt;/code&gt; transport:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;FETCH_SPLIT_REQUEST: master -&amp;gt; worker. Asks the worker to fetch a split.&lt;/li&gt;
  &lt;li&gt;FETCH_SPLIT_RESPONSE: worker -&amp;gt; master. The worker communicates the result of the operation to the master.&lt;/li&gt;
  &lt;li&gt;HEARTBEAT_REQUEST: master -&amp;gt; worker. Send to the worker to make sure it is alive.&lt;/li&gt;
  &lt;li&gt;HEARTBEAT_RESPONSE: worker -&amp;gt; master. Send response back to the master.&lt;/li&gt;
  &lt;li&gt;REGISTRATION: worker -&amp;gt; master. Register worker into the master.&lt;/li&gt;
  &lt;li&gt;SHUTDOWN: master -&amp;gt; worker. Used to shut down the worker.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The application works as follows. First the workers register with the master and at that point the master starts sending split requests to them. At a periodic interval the master sends heartbeats to check that the workers are still alive. If a worker doesn’t reply to a heartbeat the master removes the worker and re-sends the outstanding requests to other workers that are alive.
This continues until all the splits are complete.&lt;/p&gt;

&lt;h3 id=&quot;master-architecture&quot;&gt;Master architecture&lt;/h3&gt;
&lt;p&gt;We divide the master into 2 components: &lt;code&gt;master&lt;/code&gt; and &lt;code&gt;transport&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/arch.png&quot; alt=&quot;Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;master&lt;/code&gt; is in charge of sending &lt;code&gt;Fetch Split Requests&lt;/code&gt;, sending &lt;code&gt;heartbeats&lt;/code&gt; and processing &lt;code&gt;responses&lt;/code&gt; from workers.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;transport&lt;/code&gt; is in charge of sending and receiving the messages.&lt;/p&gt;

&lt;p&gt;To make the design cleaner we run &lt;code&gt;master&lt;/code&gt; and &lt;code&gt;transport&lt;/code&gt; in different threads and communicate them using a thread-safe producer consumer queue. Using different threads makes the application more responsive. E.g. if the master is taking a long time generating requests or processing responses the transport can still send and receive messages. This is the master’s pseudocode:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;splits = splitFile(filename)
while (splits.size() &amp;gt; 0) {
  now = get_time();
  if ((now - lastSplitResponseTime) &amp;gt; timeoutInterval) errorAndExit();
  if ((now - lastheartbeatSendTime) &amp;gt; heartbeatSendInterval) sendHeartbeat();
  if ((now - lastHeartbeatCheckTime) &amp;gt; hearbeatCheckInterval) checkHeartbeats();
  if (requestQueue.hasFreeSlots()) sendSplit();
  if (responseQueue.hasValidData()) processResponse();
}
shutdown(workers);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the queues are not blocking. This allows us to keep sending requests even if there are no responses and viceversa.&lt;/p&gt;

&lt;p&gt;At a certain point in time the queues will look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/queues.png&quot; alt=&quot;Queues&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is just a very simple design. One possible problem is that the heartbeat requests/responses may be delayed if there are too many Fetch Splits requests/responses in the queue. Another option would be to have a separate queue with higher priority for heartbeats (i.e. &lt;code&gt;transport&lt;/code&gt; would always empty the heartbeat queue first).
Another option is to have a whole separate &lt;code&gt;zeromq&lt;/code&gt; socket for heartbeats.&lt;/p&gt;

&lt;h3 id=&quot;transport-architecture&quot;&gt;Transport architecture&lt;/h3&gt;

&lt;p&gt;The transport layer is responsible for sending and receiving messages. As in the master case is not blocking:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while (1) {
  hasResponses = poll_for_responses();
  if (hasResponses) responseQueue.add(response);
  if (requestqueue.hasValiData()) sendToWorker(data);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;worker-architecture&quot;&gt;Worker architecture&lt;/h3&gt;
&lt;p&gt;The worker is very simple. It receives requests, does the work and sends a response in a closed loop.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while (1) {
  req = block_for_request();
  if(req == shutdown) break;
  rsp = doWork(rep);
  sendToMaster(rsp);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The complete code is on &lt;a href=&quot;https://github.com/jorgemarsal/distributor&quot;&gt;Github&lt;/a&gt; in case you want to take a closer look ;).&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;When you have a big problem scaling out across a cluster of commodity machines can be a good option.&lt;/p&gt;

&lt;p&gt;In big clusters machines fail so we need a way to check for failures and react to them.&lt;/p&gt;

&lt;p&gt;In this example the master is not fault tolerant. We could implement a fault tolerant system using something like  &lt;a href=&quot;https://github.com/logcabin/logcabin&quot;&gt;Logcabin&lt;/a&gt; or &lt;a href=&quot;https://zookeeper.apache.org/&quot;&gt;Zookeeper&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Sun, 17 May 2015 00:00:00 -0700</pubDate>
        <link>http://jorgemarsal.github.io/blog/2015/05/17/design-of-a-simple-distributed-application.html</link>
        <guid isPermaLink="true">http://jorgemarsal.github.io/blog/2015/05/17/design-of-a-simple-distributed-application.html</guid>
        
        
      </item>
    
      <item>
        <title>Introducing Distributed R in the Silicon Valley Machine Learning Meetup</title>
        <description>&lt;p&gt;A few days ago we introduced Distributed R in the Silicon Valley Machine Learning Meetup (&lt;a href=&quot;http://www.meetup.com/Silicon-Valley-Machine-Learning/events/221425830/&quot;&gt;link&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Distributed R parallelizes several R machine learning algorithms across multiple cores and machines in a cluster. So far we have implementations for linear/logistic regression, k-means, Random Forests and Page Rank. If you work with big data in R you should definitely check it out. It’s open source and available on &lt;a href=&quot;https://github.com/vertica/DistributedR&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/CRGgaGllhWzp5Y&quot; width=&quot;425&quot; height=&quot;355&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen=&quot;&quot;&gt; &lt;/iframe&gt;
&lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;strong&gt; &lt;a href=&quot;//www.slideshare.net/JorgeMartinez223/distributed-r-the-next-generation-platform-for-predictive-analytics&quot; title=&quot;Distributed R: The Next Generation Platform for Predictive Analytics&quot; target=&quot;_blank&quot;&gt;Distributed R: The Next Generation Platform for Predictive Analytics&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&quot;//www.slideshare.net/JorgeMartinez223&quot; target=&quot;_blank&quot;&gt;Jorge Martinez de Salinas&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
</description>
        <pubDate>Wed, 15 Apr 2015 00:00:00 -0700</pubDate>
        <link>http://jorgemarsal.github.io/blog/2015/04/15/introducing-distributed-r-in-the-silicon-valley-machine-learning-meetup.html</link>
        <guid isPermaLink="true">http://jorgemarsal.github.io/blog/2015/04/15/introducing-distributed-r-in-the-silicon-valley-machine-learning-meetup.html</guid>
        
        
      </item>
    
      <item>
        <title>Tracking memory usage in Linux</title>
        <description>&lt;p&gt;When working with big data optimizing the memory footprint is important.&lt;/p&gt;

&lt;p&gt;In this example we’re serializing a data frame with 50 million elements using R’s native &lt;code&gt;serialize&lt;/code&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;df &amp;lt;- data.frame(runif(50e6,1,10))
ser &amp;lt;- serialize(df,NULL)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each element is a double that takes 8 bytes. If we do the math the data frame should be 400MB (50M elements, 8byte each). The serialized version should be around 400MB too. However if we run that code and check the memory usage we see the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /proc/15155/status |grep Vm
...
VmHWM:	 1207792 kB
VmRSS:	  817272 kB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;VmRSS&lt;/code&gt; is the resident memory and in this case it’s around 800MB as we’d expect. However the peak memory usage (&lt;code&gt;VmHWM&lt;/code&gt;) is 1.2GB. Let’s fire up &lt;code&gt;GDB&lt;/code&gt; and see what’s going on. The relevant R code is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;InitMemOutPStream(&amp;amp;out, &amp;amp;mbs, type, version, hook, fun);
R_Serialize(object, &amp;amp;out);
val =  CloseMemOutPStream(&amp;amp;out);
...
return val;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we set a breakpoint right after &lt;code&gt;R_serialize&lt;/code&gt; we see that the memory usage is around 800MB, as we’d expect:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;VmRSS:	  816664 kB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However is we step into &lt;code&gt;CloseMemOutPStream&lt;/code&gt; we see this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PROTECT(val = allocVector(RAWSXP, mb-&amp;gt;count));
memcpy(RAW(val), mb-&amp;gt;buf, mb-&amp;gt;count);
free_mem_buffer(mb);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code is allocating a whole new buffer and copying the serialized object there. If we set a breakpoint just before &lt;code&gt;free&lt;/code&gt;, the memory usage at that point is 1.2GB.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;VmRSS:	 1207384 kB
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;optimizing-the-code&quot;&gt;Optimizing the code&lt;/h3&gt;

&lt;p&gt;Using R’s &lt;code&gt;serialize&lt;/code&gt; we need 3x the amount of original memory (1.2GB for a 400MB) data frame, which is not acceptable.&lt;/p&gt;

&lt;p&gt;Ideally we’d like to avoid the last copy and just serialize the object in a final buffer.&lt;/p&gt;

&lt;p&gt;The other improvement would be to serialize the object in chunks. E.g. we could have a 10MB buffer and stream parts of the serialized object from that buffer. In this case the peak memory usage would be 410MB instead of 1.2GB!&lt;/p&gt;
</description>
        <pubDate>Mon, 16 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://jorgemarsal.github.io/blog/2015/03/16/tracking-memory-usage-in-linux.html</link>
        <guid isPermaLink="true">http://jorgemarsal.github.io/blog/2015/03/16/tracking-memory-usage-in-linux.html</guid>
        
        
      </item>
    
      <item>
        <title>An interesting bug</title>
        <description>&lt;p&gt;The other day I hit an interesting bug and I thought it would be fun to share it.&lt;/p&gt;

&lt;p&gt;The interesting part is that it only happens with some compiler versions.  E.g. when compiled with GCC 4.8.2 on Ubuntu the software hangs. When compiled with GCC 4.6.4 it doesn’t. Let’s see what’s going on:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(distributedR)
distributedR_start(inst=1, log=3)
Workers registered - 1/1.
All 1 workers are registered.
Master address:port - 127.0.0.1:50002
&amp;lt;hung&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The prompt is not coming back. If we run &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;R-executor-bin&lt;/code&gt; is stuck using 100% of a core.&lt;/p&gt;

&lt;h2 id=&quot;debugging&quot;&gt;Debugging&lt;/h2&gt;

&lt;p&gt;Let’s see where is stuck. First we need the pid of &lt;code&gt;R-executor-bin&lt;/code&gt; which in this case is &lt;code&gt;57725&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gdb bin/R-executor-bin 57725
...
0x0000000000443e55 in dataptr (x=0x6509e60) at  /usr/local/lib/R/site-library/Rcpp/include/Rcpp/routines.h:197
197	inline void* dataptr(SEXP x){
(gdb) bt
#0  0x0000000000443e55 in dataptr (x=0x6509e60) at /usr/local/lib/R/site-library/Rcpp/include/Rcpp/routines.h:197
#1  0x000000000043aee0 in r_vector_start&amp;lt;24&amp;gt; (x=0x6509e60) at /usr/local/lib/R/site-library/Rcpp/include/Rcpp/internal/r_vector.h:32
#2  update (v=&amp;lt;synthetic pointer&amp;gt;, this=&amp;lt;synthetic pointer&amp;gt;) at /usr/local/lib/R/site-library/Rcpp/include/Rcpp/vector/traits.h:40
#3  update (this=&amp;lt;synthetic pointer&amp;gt;) at /usr/local/lib/R/site-library/Rcpp/include/Rcpp/vector/Vector.h:428
#4  set__ (x=&amp;lt;optimized out&amp;gt;, this=&amp;lt;synthetic pointer&amp;gt;) at /usr/local/lib/R/site-library/Rcpp/include/Rcpp/storage/PreserveStorage.h:22
#5  Vector (size=@0x7fffb4f70bc0: 38, this=&amp;lt;synthetic pointer&amp;gt;) at /usr/local/lib/R/site-library/Rcpp/include/Rcpp/vector/Vector.h:107
#6  ReadRawArgs (R=...) at platform/executor/src/executor.cpp:474
#7  0x0000000000436659 in main (argc=&amp;lt;optimized out&amp;gt;, argv=&amp;lt;optimized out&amp;gt;) at platform/executor/src/executor.cpp:709
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The process is stuck in the &lt;code&gt;dataptr&lt;/code&gt; function. GDB tell me is this one:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Rcpp/routines.h:197&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;197	inline void* dataptr(SEXP x){
198	    typedef void* (*Fun)(SEXP) ;
199	    static Fun fun = GET_CALLABLE(&quot;dataptr&quot;) ;
200	    return fun(x) ;
201	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The function is calling itself in an infinite loop! This code doesn’t make any sense. The function above is used to call &lt;code&gt;dataptr&lt;/code&gt; at runtime.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;GET_CALLABLE&lt;/code&gt; is a macro pointing to &lt;code&gt;R_GetCCallable&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#define GET_CALLABLE(__FUN__) (Fun) R_GetCCallable( &quot;Rcpp&quot;, __FUN__ )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s see what’s &lt;code&gt;R_GetCCallable&lt;/code&gt;. It’s defined in &lt;code&gt;Rdynload.h&lt;/code&gt; in R:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/* Experimental interface for exporting and importing functions from
one package for use from C code in a package.  The registration
part probably ought to be integrated with the other registrations.
The naming of these routines may be less than ideal. */

void R_RegisterCCallable(const char *package, const char *name, DL_FUNC fptr);
DL_FUNC R_GetCCallable(const char *package, const char *name);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The corresponding registration part is in &lt;code&gt;Rcpp_init.cpp:109&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RCPP_REGISTER(dataptr)
#define RCPP_REGISTER(__FUN__) R_RegisterCCallable( &quot;Rcpp&quot;, #__FUN__ , (DL_FUNC)__FUN__ );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But wait. That registration is not pointing to &lt;code&gt;dataptr&lt;/code&gt; in &lt;code&gt;Rcpp/routines.h:197&lt;/code&gt;. It’s pointing to another &lt;code&gt;dataptr&lt;/code&gt; in &lt;code&gt;Rcpp/src/barrier.cpp:71&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// [[Rcpp::register]]
void* dataptr(SEXP x){
    return DATAPTR(x);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That’s bad. There are 2 functions with the same name and we’re registering one but getting the other one at runtime! And to add insult to the injury the latter is calling itself in an infinite loop. But there are 2 weird things here:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;This only fails in some compiler versions!&lt;/li&gt;
  &lt;li&gt;The process is hung but is should segfault. If we keep recursing indefinitely at some time we’ll run out of stack space and the process will be killed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s try to find out the reason for #1. Let’s compile with both GCC 4.6.4 and GCC 4.8.2 and see the differences in the symbol tables:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nm --dynamic R-executor-bin-4.8 |grep dataptr
0000000000443e50 W _Z7dataptrP7SEXPREC
00000000006c6060 u _ZGVZ7dataptrP7SEXPRECE3fun
00000000006c5fe8 u _ZZ7dataptrP7SEXPRECE3fun

$ nm --dynamic  R-executor-bin-4.6 |grep dataptr
00000000006cafa0 u _ZGVZ7dataptrP7SEXPRECE3fun
00000000006cafa8 u _ZZ7dataptrP7SEXPRECE3fun
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Aha! GCC 4.8 has removed one of the &lt;code&gt;dataptr&lt;/code&gt; definitions (the wrong one) so we’re getting what we expect at runtime. Unfortunately GCC 4.6 has not.&lt;/p&gt;

&lt;h2 id=&quot;reproducing-the-hang-with-a-small-program&quot;&gt;Reproducing the hang with a small program&lt;/h2&gt;

&lt;p&gt;Let’s try to reproduce it with a small example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;RInside.h&amp;gt;
#include &amp;lt;iostream&amp;gt;

int main() {
    RInside R(0, 0);
    int size = 38;
    Rcpp::RawVector raw(size);
    std::cout &amp;lt;&amp;lt; &quot;done&quot; &amp;lt;&amp;lt; std::endl;
    return 0;
}

$ g++-4.6 -I/usr/local/lib/R/site-library/RInside/include/ -I/usr/local/lib/R/site-library/Rcpp/include/ -I/usr/share/R/include/ -O0 -g  -o rcpphang-4.6 rcpphang.cpp -lR -lRInside -L/usr/local/lib/R/site-library/RInside/lib/
$ g++-4.8 -I/usr/local/lib/R/site-library/RInside/include/ -I/usr/local/lib/R/site-library/Rcpp/include/ -I/usr/share/R/include/ -O0 -g  -o rcpphang-4.8 rcpphang.cpp -lR -lRInside -L/usr/local/lib/R/site-library/RInside/lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s run it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./rcpphang-4.8
done
$ ./rcpphang-4.6
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait, what? This is working! No hang. Let’s take a look at the symbol tables:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nm --dynamic rcpphang-4.6 |grep dataptr
&amp;lt;no output&amp;gt;

$ nm --dynamic rcpphang-4.8 |grep dataptr
&amp;lt;no output&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, that’s why it’s working. &lt;code&gt;dataptr&lt;/code&gt; is not in the dynamic symbol tables. Looking at the original SW (that hung) I see we compile with &lt;code&gt;-rdynamic&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;-rdynamic&lt;/strong&gt;: Pass the flag -export-dynamic to the ELF linker, on targets that support it. This instructs the linker to add all symbols, not only used ones, to the dynamic symbol table. This option is needed for some uses of dlopen or to allow obtaining backtraces from within a program.&lt;/p&gt;

&lt;p&gt;Let’s compile with &lt;code&gt;-rdynamic&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ g++-4.6 -I/usr/local/lib/R/site-library/RInside/include/ -I/usr/local/lib/R/site-library/Rcpp/include/ -I/usr/share/R/include/ -O0 -g -rdynamic -o rcpphang-4.6 rcpphang.cpp -lR -lRInside -L/usr/local/lib/R/site-library/RInside/lib/
$ g++-4.8 -I/usr/local/lib/R/site-library/RInside/include/ -I/usr/local/lib/R/site-library/Rcpp/include/ -I/usr/share/R/include/ -O0 -g  -o rcpphang-4.8 rcpphang.cpp -lR -lRInside -L/usr/local/lib/R/site-library/RInside/lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The symbols are there now:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nm --dynamic rcpphang-4.6-rdyn-noopt|grep dataptr
00000000004039f8 W _Z7dataptrP7SEXPREC
0000000000606668 u _ZGVZ7dataptrP7SEXPRECE3fun
0000000000606670 u _ZZ7dataptrP7SEXPRECE3fun

$ nm --dynamic rcpphang-4.8-rdyn-noopt|grep dataptr
0000000000403a08 W _Z7dataptrP7SEXPREC
0000000000606648 u _ZGVZ7dataptrP7SEXPRECE3fun
0000000000606650 u _ZZ7dataptrP7SEXPRECE3fun

$ ./rcpphang-4.8
&amp;lt;segfault&amp;gt;

$ ./rcpphang-4.6
&amp;lt;segfault&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, now both have the symbol and therefore both segfault! The other difference is that we’re compiling the original SW with &lt;code&gt;-O2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ g++-4.6 -I/usr/local/lib/R/site-library/RInside/include/ -I/usr/local/lib/R/site-library/Rcpp/include/ -I/usr/share/R/include/ -O2 -g -rdynamic -o rcpphang-4.6 rcpphang.cpp -lR -lRInside -L/usr/local/lib/R/site-library/RInside/lib/
$ g++-4.8 -I/usr/local/lib/R/site-library/RInside/include/ -I/usr/local/lib/R/site-library/Rcpp/include/ -I/usr/share/R/include/ -O2 -g  -o rcpphang-4.8 rcpphang.cpp -lR -lRInside -L/usr/local/lib/R/site-library/RInside/lib/


$ nm --dynamic rcpphang-4.8|grep dataptr
00000000006046a8 u _ZGVZ7dataptrP7SEXPRECE3fun
00000000006046a0 u _ZZ7dataptrP7SEXPRECE3fun
$ nm --dynamic rcpphang-4.6|grep dataptr
0000000000402b50 W _Z7dataptrP7SEXPREC
00000000006046b8 u _ZGVZ7dataptrP7SEXPRECE3fun
00000000006046c0 u _ZZ7dataptrP7SEXPRECE3fun
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have the same behavior as with the original SW! GCC 4.8 has removed the symbol while 4.6 hasn’t. That’s why 4.8 works and 4.6 hangs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./rcpphang-4.8
done

$ ./rcpphang-4.6
&amp;lt;hung&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now regarding weirdness #2, the explanation for the hang instead of the segfault is probably that the compiler is doing &lt;a href=&quot;http://c2.com/cgi/wiki?TailCallOptimization&quot;&gt;tail call optimization&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;fixing-the-problem&quot;&gt;Fixing the problem&lt;/h2&gt;

&lt;p&gt;One way to fix this issue is to make sure the conflicting function is not exported in the shared library. We can achieve that adding the following attribute:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;inline __attribute__ ((visibility (&quot;hidden&quot;))) void* dataptr(SEXP x){
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end we reported the bug to the &lt;code&gt;Rcpp&lt;/code&gt; author and he applied the fix in &lt;code&gt;Rcpp 0.11.5&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;That was fun :)&lt;/p&gt;
</description>
        <pubDate>Mon, 16 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://jorgemarsal.github.io/blog/2015/03/16/an-interesting-bug.html</link>
        <guid isPermaLink="true">http://jorgemarsal.github.io/blog/2015/03/16/an-interesting-bug.html</guid>
        
        
      </item>
    
  </channel>
</rss>
