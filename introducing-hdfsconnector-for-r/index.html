<!DOCTYPE html>
<html>
  <head>
    <title>Jorge Martinez de Salinas</title>
    <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
    <meta name="description" content="Jorge Martinez de Salinas">
    <meta property="og:description" content="Jorge Martinez de Salinas" />
    <meta name="author" content="Jorge Martinez de Salinas" />

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="alternate" type="application/rss+xml" title="Jorge Martinez de Salinas" href="//feed.xml" />
  </head>

  <body>
    <div class="container">

      <nav class="navbar navbar-default">
        <div class="container-fluid">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Jorge Martinez de Salinas</a>
          </div>
          <div class="collapse navbar-collapse" id="myNavbar">
            <ul class="nav navbar-nav">
              <li><a href="/tech/">Tech</a></li>
              <li><a href="/biz/">Biz</a></li>
              <li><a href="/books/">Books</a></li>
              <li><a href="/essays/">Essays</a></li>
              <li><a href="https://github.com/jorgemarsal"><i class="fa fa-lg fa-github-square"></i><span class="icon-label">GitHub</span></a></li>
              <li><a href="https://twitter.com/jorgemarsal"><i class="fa fa-lg fa-twitter-square"></i><span class="icon-label">Twitter</span></a></li>
              <li><a href="https://linkedin.com/in/jorgemarsal"><i class="fa fa-lg fa-linkedin-square"></i><span class="icon-label">LinkedIn</span></a></li>
            </ul>
          </div>
        </div>
      </nav> <!--navbar-->
      <article class="post">
  <h1>Introducing hdfsconnector for R</h1>

  <div class="entry">
    <h3 id="introduction">Introduction</h3>
<p>Today we’re excited to introduce a new <a href="https://github.com/vertica/r-dataconnector">connector</a> for Distributed R that enables R users to read CSV and ORC files from HDFS. One of its main features is the ability to parallelize file loads across multiple machines, drastically improving performance. This post gives an overview of how the connector was designed.</p>

<h3 id="parallelizing-file-load">Parallelizing file load</h3>
<p>To reduce the time it takes to load a big file the connector splits the loading across different machines in the cluster.</p>

<p>To understand how this works let’s walk through an example. Say we have a 4GB CSV file (test4GB.csv) and a single machine with 4 executors (4 cores). The connector creates as many partitions as executors. In this case each core will load 1GB of the CSV file. When working with ORC files we don’t have that much flexibility because the level of parallelism is already defined (parallelism == number of stripes in the file). In the ORC case we’ll assign different stripes to different executors.</p>

<p>Once we’ve split the file into multiple chunks we assign each chunk to the best executor. For files stored in HDFS we try to minimize data movement in the cluster. This means we’ll assign the chunk to the executor that has the highest number of HDFS blocks locally. In this case executor 1 has the highest number of blocks corresponding to chunk 1 (red), executor 2 has the highest number of blocks for chunk 2 and so on.</p>

<p><img src="/assets/hdfsconnector/scheduling.png" alt="Scheduling" /></p>

<p>After creating the scheduling map the master sends a message to the executors indicating which chunks should be loaded on which executor. In this case the master sends 1 message to each executor:</p>

<ul>
  <li>To executor 1: load test4GB.csv, range: [0,   1GB), schema …</li>
  <li>To executor 2: load test4GB.csv, range: [1GB, 2GB), schema …</li>
  <li>To executor 3: load test4GB.csv, range: [2GB, 3GB), schema …</li>
  <li>To executor 4: load test4GB.csv, range: [3GB, 4GB), schema …</li>
</ul>

<h3 id="loading-individual-chunks-on-the-executors">Loading individual chunks on the executors</h3>

<p>Each executor implements a loading stack made of 4 layers (Assembler, Record Parser, Split Producer and Block Reader). To understand what each layer does it’s useful to start from the bottom. The file is just an array of bytes. We extract splits from the byte array (e.g. lines in CSV). Then we extract individual records from that split (fields in that line). Finally we assemble a final object out of individual records (e.g. an R dataframe).</p>

<p><img src="/assets/hdfsconnector/loading_stack.png" alt="Loading stack" /></p>

<p>In the next paragraphs we give more details about how the different layers work:</p>

<h4 id="block-reader">Block reader</h4>
<p>Reads a stream of bytes from storage. We have 2 flavors (<code class="highlighter-rouge">LocalBlockReader</code> and <code class="highlighter-rouge">HdfsBlockReader</code>).</p>

<p><code class="highlighter-rouge">LocalBlockReader</code> reads from the local FS while <code class="highlighter-rouge">HdfsBlockReader</code> reads from HDFS. We’re using the WebHDFS interface but we may decide to switch to the native interface in the future if HTTP is not fast enough.</p>

<p>To get the best performance we have an I/O thread that pre-fetches blocks while another thread parses the previous block. The idea is to overlap I/O with compute for improved performance.</p>

<p><img src="/assets/hdfsconnector/pipeline.png" alt="Pipeline" /></p>

<h4 id="split-producer">Split producer</h4>
<p>Assembles splits (lines in CSV), stripes in ORC, etc. For that it might need to read one or more blocks.
We have 2 flavors <code class="highlighter-rouge">DelimiterSplitProducer</code> and <code class="highlighter-rouge">OffsetSplitProducer</code>.</p>

<p>In CSV lines are usually determined by a newline character (\n). In this case we use the <code class="highlighter-rouge">DelimiterSplitProducer</code> to extract the lines. This is quite slow as we have to look at every byte to detect newline characters. Other formats (such as ORC) include the split boundaries in the metadata and we can extract the splits directly using the <code class="highlighter-rouge">OffsetSplitProducer</code>.</p>

<p>An additional complication for CSV files is that the offset from which we start reading may not correspond to the beginning of the line (actually this is the normal case). To handle this we have to backtrack one character and then skip that incomplete line. The discarded line will be processed by the executor that’s working on the previous chunk.</p>

<p>To understand this better let’s use an example. Suppose executor 1 is assigned range [0, 6] and executor 2  range [6,12]. Lines correspond to ranges [0,4),[4,8) and [8,12). Executor 1 will process both lines 1 and 2. Even though the range finishes at offset 6 it will continue until the full 2nd line is consumed. Executor 2 in turn will discard the incomplete line and only process line 3.</p>

<p><img src="/assets/hdfsconnector/csv_lines.png" alt="CSV line splitting" /></p>

<h4 id="record-parser">Record parser</h4>
<p>Breaks a split into individual records.
The record parser is format dependent. In the CSV case we use a separator (usually a comma) to separate the records.
ORC is a columnar format so all the records in the same column are stored together in a highly optimized way (encoded and compressed).</p>

<p>To understand the importance of the format let’s see how an int column is encoded both in CSV and ORC.
Let’s say the data is 1000,1001,1002 …
In CSV each record takes 4 bytes or 32 bits (let’s assume the numbers fit between 0 and 9999). In ORC we’d only need 14 bits to encode each number.
On top of that we can add smart encodings. For example in this case instead of storing the numbers we can store the delta between each consecutive number. That would be [1,1,1,1 …]. This can be run-length encoded to [num-elements,1]. Finally we can apply compression to reduce the final size even more.
ORC supports many different encodings (e.g. dictionary encoding for string type) and different compression types.</p>

<p>Another advantage of ORC vs CSV is that it supports complex types (struct, map, list …)</p>

<h4 id="assembler">Assembler</h4>
<p>The final layer assembles a number of records into a final object (e.g. an R dataframe).</p>

<p>The mapping in CSV is straightforward. In the ORC case is more complex as it supports more types and they can be arbitrarily nested. The mapping we use is:</p>

<ul>
  <li>ORC Struct -&gt; R dataframe</li>
  <li>ORC List -&gt; R list</li>
  <li>ORC Map -&gt; R dataframe with 2 columns (key, value).</li>
</ul>

<h3 id="performance">Performance</h3>
<p>Performance is highly dependent on the HW configurations (numbers or cores, performance per core, bisection bandwidth between the HDFS nodes and the nodes loading the files, etc.).</p>

<p>In the graph we present the results of loading a 64MB CSV file with a single machine (4 cores). The time decreases linearly as we add cores.</p>

<p><img src="/assets/hdfsconnector/perf.png" alt="Performance" /></p>

<h3 id="conclusion">Conclusion</h3>
<p>This new connector allows R users to read files from HDFS. It can parallelize file loads across cores and machines which results in great performance.</p>

<p>We offer this functionality as part of <a href="https://github.com/vertica/distributedr">Distributed R</a> (supporting distributed operation) and also as an individual package named <a href="https://github.com/vertica/r-dataconnector">dataconnector</a>.</p>

<p>The connector is open-source and we encourage feedback and contributions.</p>

  </div>

  <div class="date">
    Written on October 24, 2015
  </div>

  <div class="twitter-plug">
  <a href="https://twitter.com/jorgemarsal" class="twitter-follow-button" data-show-count="false">Follow @jorgemarsal</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>


  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'jorgemarsal';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</article>

      <footer>
         <div class="container">
            <hr>
            <div class="row">
               <div class="col-xs-10">&copy; 2017 Jorge Martinez de Salinas         </div>
               <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
            </div>
         </div>
      </footer>
    </div>
    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-45855395-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/introducing-hdfsconnector-for-r/',
		  'title': 'Introducing hdfsconnector for R'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
