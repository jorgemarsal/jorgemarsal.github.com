<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="http://jekyllrb.com" version="3.4.0">Jekyll</generator><link href="http://jorgemarsal.github.io/blog//feed.xml" rel="self" type="application/atom+xml" /><link href="http://jorgemarsal.github.io/blog//" rel="alternate" type="text/html" /><updated>2017-03-19T11:14:34-07:00</updated><id>http://jorgemarsal.github.io/blog//</id><title type="html">Jorge Martinez</title><entry><title type="html">Web scalability for startup engineers</title><link href="http://jorgemarsal.github.io/blog//web-scalability-for-startup-engineers/" rel="alternate" type="text/html" title="Web scalability for startup engineers" /><published>2017-02-20T00:00:00-08:00</published><updated>2017-02-20T00:00:00-08:00</updated><id>http://jorgemarsal.github.io/blog//web-scalability-for-startup-engineers</id><content type="html" xml:base="http://jorgemarsal.github.io/blog//web-scalability-for-startup-engineers/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">The effective engineer</title><link href="http://jorgemarsal.github.io/blog//the-effective-engineer/" rel="alternate" type="text/html" title="The effective engineer" /><published>2017-02-20T00:00:00-08:00</published><updated>2017-02-20T00:00:00-08:00</updated><id>http://jorgemarsal.github.io/blog//the-effective-engineer</id><content type="html" xml:base="http://jorgemarsal.github.io/blog//the-effective-engineer/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Running lean book review</title><link href="http://jorgemarsal.github.io/blog//running-lean/" rel="alternate" type="text/html" title="Running lean book review" /><published>2017-02-20T00:00:00-08:00</published><updated>2017-02-20T00:00:00-08:00</updated><id>http://jorgemarsal.github.io/blog//running-lean</id><content type="html" xml:base="http://jorgemarsal.github.io/blog//running-lean/">&lt;p&gt;With the advent of the Internet, cloud computing, and open 
source software, the cost of building products is at an 
all-time low. Yet, the odds of building successful startups 
haven’t improved much.&lt;/p&gt;

&lt;p&gt;Running Lean is a systematic process for iterating from Plan A 
to a plan that works, before running out of resources.&lt;/p&gt;

&lt;p&gt;the Minimum Viable Product is the smallest thing you can build 
that will create the value you’ve promised to your market.&lt;/p&gt;

&lt;p&gt;Key Saas metrics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Attention&lt;/li&gt;
  &lt;li&gt;Stickiness&lt;/li&gt;
  &lt;li&gt;Conversion&lt;/li&gt;
  &lt;li&gt;Revenue per customer&lt;/li&gt;
  &lt;li&gt;Customer acquisition cost&lt;/li&gt;
  &lt;li&gt;Virality&lt;/li&gt;
  &lt;li&gt;Upselling&lt;/li&gt;
  &lt;li&gt;Uptime and reliability&lt;/li&gt;
  &lt;li&gt;Churn&lt;/li&gt;
  &lt;li&gt;Lifetime value&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="books" /><summary type="html">With the advent of the Internet, cloud computing, and open source software, the cost of building products is at an all-time low. Yet, the odds of building successful startups haven’t improved much.</summary></entry><entry><title type="html">Lean analytics review</title><link href="http://jorgemarsal.github.io/blog//lean-analytics-review/" rel="alternate" type="text/html" title="Lean analytics review" /><published>2017-02-20T00:00:00-08:00</published><updated>2017-02-20T00:00:00-08:00</updated><id>http://jorgemarsal.github.io/blog//lean-analytics-review</id><content type="html" xml:base="http://jorgemarsal.github.io/blog//lean-analytics-review/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">High output management review</title><link href="http://jorgemarsal.github.io/blog//high-output-management-review/" rel="alternate" type="text/html" title="High output management review" /><published>2017-02-20T00:00:00-08:00</published><updated>2017-02-20T00:00:00-08:00</updated><id>http://jorgemarsal.github.io/blog//high-output-management-review</id><content type="html" xml:base="http://jorgemarsal.github.io/blog//high-output-management-review/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Flow review</title><link href="http://jorgemarsal.github.io/blog//flow-review/" rel="alternate" type="text/html" title="Flow review" /><published>2017-02-20T00:00:00-08:00</published><updated>2017-02-20T00:00:00-08:00</updated><id>http://jorgemarsal.github.io/blog//flow-review</id><content type="html" xml:base="http://jorgemarsal.github.io/blog//flow-review/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Designing data intensive applications</title><link href="http://jorgemarsal.github.io/blog//designing-data-intensive-applications/" rel="alternate" type="text/html" title="Designing data intensive applications" /><published>2017-02-20T00:00:00-08:00</published><updated>2017-02-20T00:00:00-08:00</updated><id>http://jorgemarsal.github.io/blog//designing-data-intensive-applications</id><content type="html" xml:base="http://jorgemarsal.github.io/blog//designing-data-intensive-applications/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Correlating application logs and resource utilization using the ELK stack</title><link href="http://jorgemarsal.github.io/blog//correlating-application-logs-and-resource-utilization-using-the-elk-stack/" rel="alternate" type="text/html" title="Correlating application logs and resource utilization using the ELK stack" /><published>2016-04-22T00:00:00-07:00</published><updated>2016-04-22T00:00:00-07:00</updated><id>http://jorgemarsal.github.io/blog//correlating-application-logs-and-resource-utilization-using-the-elk-stack</id><content type="html" xml:base="http://jorgemarsal.github.io/blog//correlating-application-logs-and-resource-utilization-using-the-elk-stack/">&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;
&lt;p&gt;Sometimes when running an application in production it’s useful to correlate events from different sources (e.g. application logs with resource utilization data). This can be useful for instance to discover requests that take a disproportionate amount of resources.
In this post we explore how to implement this in practice using the &lt;a href=&quot;https://www.elastic.co/webinars/introduction-elk-stack&quot;&gt;ELK stack&lt;/a&gt;. The &lt;a href=&quot;http://github.com/jorgemarsal/elkdemo&quot;&gt;full source code&lt;/a&gt; is available for those of you that want to dig deeper.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-the-data-pipeline&quot;&gt;Setting up the data pipeline&lt;/h2&gt;

&lt;p&gt;To collect resource utilization data we’ll use &lt;a href=&quot;http://dag.wiee.rs/home-made/dstat/&quot;&gt;dstat&lt;/a&gt; and expose the data through a web interface:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ curl -s localhost:8888|python -m json.tool
{
    &quot;cpu&quot;: {
        &quot;hiq&quot;: &quot;0&quot;,
        &quot;idl&quot;: &quot;99&quot;,
        &quot;siq&quot;: &quot;0&quot;,
        &quot;sys&quot;: &quot;0&quot;,
        &quot;usr&quot;: &quot;1&quot;,
        &quot;wai&quot;: &quot;0&quot;
    },
    &quot;disk&quot;: {
        &quot;read&quot;: &quot;0&quot;,
        &quot;writ&quot;: &quot;0&quot;
    },
    &quot;memory&quot;: {
        &quot;buff&quot;: &quot;340M&quot;,
        &quot;cach&quot;: &quot;5469M&quot;,
        &quot;free&quot;: &quot;2644M&quot;,
        &quot;used&quot;: &quot;11.3G&quot;
    },
    &quot;net&quot;: {
        &quot;recv&quot;: &quot;108B&quot;,
        &quot;send&quot;: &quot;74B&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We can poll this information periodically and send it to the ELK pipeline (Logstash is listening for input on port 5000):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ while true; \
&amp;gt; do curl -s localhost:8888 | \
&amp;gt; python3 dstat.py | \
&amp;gt; jq '.' -c -M | \
&amp;gt; nc localhost 5000; \
&amp;gt; sleep 1; \
&amp;gt; done
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;At this point we’re successfully collecting resource utilization data. The next step is to collect the application logs. To simulate a real application we’ve created a toy Go application that burns CPU for a configurable period of time:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func spin(period int) {
    b, err := json.Marshal(Message{fmt.Sprintf(&quot;Spinning for %d secs&quot;, period)})
    if err == nil {
            shipToLogstash(append(b, []byte(&quot;\n&quot;)...))
    }
    now := time.Now()
    end := now.Add(time.Duration(period) * time.Second)
    doSpin := func() {
            for time.Now().Before(end) {
            }
    }
    for i := 0; i &amp;lt; 8; i++ {
            go doSpin()
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We can trigger that function by making an HTTP call like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ curl 'localhost:8080/?period=10'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;When invoked, the function produces the following log entry and burns CPU for the specified period of time:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;“Text”:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;“Spinning&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;secs”&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;correlating-application-logs-with-resource-utilization&quot;&gt;Correlating application logs with resource utilization&lt;/h2&gt;
&lt;p&gt;We can plot the resource utilization information we’re gathering using Kibana. During normal operation the CPU is idle between 90 and 100% of the time. However as we can see in the picture below, there’s a period of approximately 10 seconds (starting around 11:53:32) in which the CPU load increases a lot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/elk/cpuload.png&quot; alt=&quot;CPU load&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One way to start investigating the root cause of this behavior is to search the time frame prior to the load spike. We can easily do that in Kibana and after a little bit of log searching we find the culprit. As we can see in the picture below there was a request to the Go application that resulted in the load spike:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/elk/log2.png&quot; alt=&quot;Log entry&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Having a good monitoring infrastructure is crucial to understand how your application behaves in production. In this post we’ve explored how to create it by leveraging the ELK stack. Although this post uses a toy example, we can use the same setup to gather relevant information for real applications.&lt;/p&gt;</content><author><name></name></author><category term="tech" /><category term="elk" /><category term="elasticsearch" /><category term="logging" /><category term="monitoring" /><category term="logstash" /><category term="kibana" /><summary type="html">Intro Sometimes when running an application in production it’s useful to correlate events from different sources (e.g. application logs with resource utilization data). This can be useful for instance to discover requests that take a disproportionate amount of resources. In this post we explore how to implement this in practice using the ELK stack. The full source code is available for those of you that want to dig deeper.</summary></entry><entry><title type="html">Going faster with Just-In-Time compilation</title><link href="http://jorgemarsal.github.io/blog//going-faster-with-just-in-time-compilation/" rel="alternate" type="text/html" title="Going faster with Just-In-Time compilation" /><published>2016-01-14T00:00:00-08:00</published><updated>2016-01-14T00:00:00-08:00</updated><id>http://jorgemarsal.github.io/blog//going-faster-with-just-in-time-compilation</id><content type="html" xml:base="http://jorgemarsal.github.io/blog//going-faster-with-just-in-time-compilation/">&lt;p&gt;This post is about code optimization using &lt;a href=&quot;https://en.wikipedia.org/wiki/Just-in-time_compilation&quot;&gt;Just-In-Time compilation&lt;/a&gt;. The main insight is that there is a tradeoff between generality and performance. In other words if you want to make things generic they will be slower than a specialized solution.&lt;/p&gt;

&lt;p&gt;Let’s use an example to make things clearer. Say we’re working on an analytics DB and we want to sum 2 columns. If we code a generic solution we cannot make any assumption about the column types, the number of rows in the columns or the hardware characteristics. A generic code would then be:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Let’s assume the column are stored in arrays a and b
// and we want to store the result in array c
if (type == ‘double) {
    for (int i = 0; i &amp;lt; nrows; ++i) {
        c[i] = a[i] + b[i];
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The compiler cannot do much to optimize this code. It has a conditional and we don’t know the value of &lt;code class=&quot;highlighter-rouge&quot;&gt;nrows&lt;/code&gt; so we cannot optimize the loop.&lt;/p&gt;

&lt;p&gt;However at runtime we’ll have more information (like number of rows, types, etc.) and we can leverage it to create more efficient code.  Let’s say we know the column type is double and we also know the hardware supports &lt;a href=&quot;https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions&quot;&gt;SSE&lt;/a&gt; instructions that can add 2 pairs of doubles in a single cycle. We can generate optimizing code on the fly like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;| mov eax, a
| movapd xmm0, oword [eax]  // load array a using SSE
| mov eax, b
| movapd xmm1, oword [eax]  // load array b using SSE
| addpd xmm0, xmm1              // add arrays a and b in a single cycle using SSE
| mov eax, c
| movapd oword [eax], xmm0  // store result in array c using SSE
| ret
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can take a look at the full code (adapted from this excellent &lt;a href=&quot;http://blog.reverberate.org/2012/12/hello-jit-world-joy-of-simple-jits.html&quot;&gt;example&lt;/a&gt;) &lt;a href=&quot;https://github.com/jorgemarsal/jitdemo/blob/master/sse.dasc&quot;&gt;here&lt;/a&gt;. We use &lt;code class=&quot;highlighter-rouge&quot;&gt;dynasm&lt;/code&gt; to generate the machine code that performs the additions using SSE. Then we copy that machine code to a memory region marked as executable and jump to it.&lt;/p&gt;

&lt;p&gt;With this optimization the performance improves dramatically. 1 million iterations of the first example run in 0.04 seconds while the second example runs in 0.002 seconds. That’s a 20X speedup!&lt;/p&gt;

&lt;p&gt;This is a pretty contrived example but a generalization of this technique is extremely useful to improve performance in real world cases and companies like &lt;a href=&quot;https://databricks.com//2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html&quot;&gt;Databricks&lt;/a&gt; use it in their products.&lt;/p&gt;

&lt;p&gt;Pretty cool huh?&lt;/p&gt;</content><author><name></name></author><category term="tech" /><summary type="html">This post is about code optimization using Just-In-Time compilation. The main insight is that there is a tradeoff between generality and performance. In other words if you want to make things generic they will be slower than a specialized solution.</summary></entry><entry><title type="html">100000-foot view of a modern web service</title><link href="http://jorgemarsal.github.io/blog//100000-foot-view-of-a-modern-web-service/" rel="alternate" type="text/html" title="100000-foot view of a modern web service " /><published>2016-01-06T00:00:00-08:00</published><updated>2016-01-06T00:00:00-08:00</updated><id>http://jorgemarsal.github.io/blog//100000-foot-view-of-a-modern-web-service</id><content type="html" xml:base="http://jorgemarsal.github.io/blog//100000-foot-view-of-a-modern-web-service/">&lt;p&gt;In this post we’re going to explore how to deliver a modern web service. To make things clearer we’re going to use a OCR service as an example. All the code is available on &lt;a href=&quot;https://github.com/jorgemarsal/webocr/tree/master/webocr&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;WebOCR&lt;/code&gt; extracts text from user uploaded images using a technique known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Optical_character_recognition&quot;&gt;OCR&lt;/a&gt;. Users provide the image information (for now just the URL) and the service downloads the image, extracts the text and sends the result back to the user.&lt;/p&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;
&lt;p&gt;The architecture is shown in this picture below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/webocr/arch.png&quot; alt=&quot;Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are 2 main elements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The WebOCR service provides a restful API for extracting text from images.&lt;/li&gt;
  &lt;li&gt;The task server and associated workers perform the actual OCR processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The basic workflow is:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Users post a request with the image information.
    &lt;ul&gt;
      &lt;li&gt;The WebOCR service sends the task to the task server and …&lt;/li&gt;
      &lt;li&gt;… replies with a 202 Accepted code and a link to check the task progress.&lt;/li&gt;
      &lt;li&gt;The task server sends the task to a queue (RabbitMQ in this case).&lt;/li&gt;
      &lt;li&gt;The worker picks up the task from the queue.&lt;/li&gt;
      &lt;li&gt;The server polls the task server periodically to update the task status and stores that information.&lt;/li&gt;
      &lt;li&gt;The client can also access this information through the &lt;code class=&quot;highlighter-rouge&quot;&gt;/queue&lt;/code&gt; endpoint in the WebOCR service.&lt;/li&gt;
      &lt;li&gt;Eventually the task will succeed or fail. If the task succeeds the client will get redirected to the result. If it fails the client can retrieve the failure cause from the /queue endpoint.&lt;/li&gt;
      &lt;li&gt;Both the client and the web server have timeouts and consider the task failed if they don’t receive a response in a timely manner.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is an example exchange:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -v -H &quot;Content-Type: application/json&quot; -X POST \
-d '{&quot;url&quot;:&quot;http://localhost:1234/static/ocr/sample1.jpg&quot;}' \
http://localhost:1234/api/v1/services
...
HTTP/1.1 202 Accepted
Content-Location: /queue/7bc6c484-df47-4cb6-b254-17b770b52060
….
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The server replies with a 202 code and a URL to query the task status. If we access that URL and the task is still pending we’ll get a 200 ok and info about the task. When the task is done we’ll get a 303 and a link to the newly created resource:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -v http://localhost:1234/queue/abb5fbbf-9519-4d21-af23-e3a7da1ca480

….
HTTP/1.1 303 See Other
Location: /api/v1/service/90
…
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Let’s get the result:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ curl -v http://localhost:1234/api/v1/service/90
HTTP/1.1 200 OK
{
    &quot;result&quot;: &quot;THINGS YOU SHOULD KNOW ABOU ...
              ... neiraviers common In\n\n&quot;,
    &quot;state&quot;: &quot;SUCCESS&quot;,
    &quot;url&quot;: &quot;http://localhost:1234/static/ocr/sample1.jpg&quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We also have a websocket channel to send real time information of the task progress to the frontend.&lt;/p&gt;

&lt;p&gt;The next sections go into more detail on how all of this is implemented.&lt;/p&gt;

&lt;h3 id=&quot;backend&quot;&gt;Backend&lt;/h3&gt;
&lt;p&gt;The backend is a Tornado app that implements the restful API. There aren’t any blocking operations. We either use event-loop aware functionality (like Tornado’s &lt;code class=&quot;highlighter-rouge&quot;&gt;AsyncHTTPClient&lt;/code&gt; or run the task in a different thread with the &lt;code class=&quot;highlighter-rouge&quot;&gt;@run_on_executor&lt;/code&gt; decorator. By using this design the server can handle many concurrent user connections.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@run_on_executor
def _on_executor(self, fn, *args, **kwargs):
    &quot;&quot;&quot;Execute the given function on another thread&quot;&quot;&quot;
    return fn(*args, **kwargs)

@gen.coroutine
def f():
    http_client = AsyncHTTPClient()
    response = yield http_client.fetch(&quot;http://example.com&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;database&quot;&gt;Database&lt;/h4&gt;

&lt;p&gt;Choosing the right database is probably one of the most important decision in a web project. In this case, since the performance requirements are modest, we don’t need a database cluster and we can live with a single node and snapshot backups. In this situation we can pretty much do whatever we want and we’ve chosen MySQL. When the performance requirements are higher then we need to consider other solutions and keep in mind the associated performance and consistency tradeoffs.&lt;/p&gt;

&lt;p&gt;As mentioned earlier the backend stores the state in a MySQL database but instead of having a fixed schema we have a &lt;code class=&quot;highlighter-rouge&quot;&gt;schemaless&lt;/code&gt; design. The original idea comes from &lt;a href=&quot;https://backchannel.org//friendfeed-schemaless-mysql&quot;&gt;Friendfeed’s schemaless design&lt;/a&gt; and the implementation (with minor modifications) comes from &lt;a href=&quot;https://github.com/eklitzke/schemaless&quot;&gt;Evan Klitzke&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this design we store compressed pickled python dictionaries that can hold arbitrary data, making schema evolution simple. Note that this idea is from 2009 when NoSQL offerings weren’t as mature as today. Probably nowadays we’d be better off using something like Mongo. Storing and querying the database is done like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;row = self.db.put(dict(url=service['url'], state='PENDING'))
rows = url_index.query(c.url == url)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;task-server&quot;&gt;Task server&lt;/h4&gt;
&lt;p&gt;We’ve decided to offload the actual computations to a task server. This way we can keep a simple WebOCR service and scale the task server independently if the load increases.&lt;/p&gt;

&lt;p&gt;The task server exposes another REST API to send tasks to Celery. Celery is an asynchronous task queue/job queue based on distributed message passing. The workflow is quite simple. The WebOCR service posts tasks to the task server. The task server sends those tasks to a queue (RabbitMQ in this case) and on the other end, one or more task workers pick up the tasks.&lt;/p&gt;

&lt;h4 id=&quot;advice-for-developing-backend-code&quot;&gt;Advice for developing backend code&lt;/h4&gt;

&lt;p&gt;Giving a complete overview of all the things to keep in mind when developing backend SW is out of scope for this post. However we’ll just give a few pieces of advice.&lt;/p&gt;

&lt;p&gt;We decided to develop the service in Python because it’s a language we’re pretty familiar with. Developing in python is really fast due to its dynamic nature and it comes with libraries for almost everything. Node.js could have been a good option as well. If type safety or performance are important considerations then Go would have been a better alternative.&lt;/p&gt;

&lt;p&gt;A lot of advice about Python itself can be found in the fantastic book &lt;a href=&quot;http://www.effectivepython.com/&quot;&gt;Effective Python&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One of the most important things to do when developing software is having good testing. One rule that I find particularly useful to increase confidence is having a high test coverage. Keep in mind that high coverage is not enough though. It’s important to design a very comprehensive test plan. Otherwise you can have 100% coverage but ignore a lot of real-world cases. A tool that works well is &lt;code class=&quot;highlighter-rouge&quot;&gt;pytest-cov&lt;/code&gt;. You can get coverage numbers using this command:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;py.test --cov=webocr --cov-report=html
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As an example if we want to test a call to a remote service we should at least test a couple scenarios:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Service works ok&lt;/li&gt;
  &lt;li&gt;Service not running&lt;/li&gt;
  &lt;li&gt;Service returns HTTP errrors&lt;/li&gt;
  &lt;li&gt;Service returns 200 ok but response data is malformed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To easily test all those error conditions we can mock the service using somethind like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class App(object):
    def __init__(self, service):
        self.service = service

    def do(self):
        try:
            self.service.do(something)
        except ServiceError as e:
            handle_exception()

def test_normal():
    “”” Normal service “””
    app = App(service)

def test_error():
    “”” Mock service that triggers all the error conditions “””
    app = App(mock_service)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Correctness is absolute necessary but you should also take a look at the performance of the service. You can use something like &lt;a href=&quot;http://jmeter.apache.org&quot;&gt;Jmeter&lt;/a&gt; for that.&lt;/p&gt;

&lt;p&gt;Finally a linting tool like &lt;code class=&quot;highlighter-rouge&quot;&gt;flakes8&lt;/code&gt; can point common errors and style issues.&lt;/p&gt;

&lt;p&gt;I recommend having testing, linting and other tooling as part of the CI process both on the client (via Git hooks) and on the server (with something like Jenkins).&lt;/p&gt;

&lt;h3 id=&quot;frontend&quot;&gt;Frontend&lt;/h3&gt;

&lt;p&gt;We’ve created a simple webpage to submit new tasks and to display the status of pending/finished tasks (see pic below). On the left hand side we have a form to submit the image information. In the middle we display the original image and on the right the extracted text. We also have a websocket channel to receive real-time progress updates.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/webocr/frontend.png&quot; alt=&quot;Frontend&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m by no means a frontend expert. I just know the minimum HTML, CSS and JS to get by. Having said that, it pays off to organize the code properly. For JS we follow the MVC pattern. The important point to remember is that the model and the view don’t talk directly, they always go through the controller.&lt;/p&gt;

&lt;h3 id=&quot;infrastructure&quot;&gt;Infrastructure&lt;/h3&gt;

&lt;p&gt;Developing the software is just half the problem Then you have to reliably run it in production.
In this section I’m gonna talk about 2 great pieces of software that we use and help greatly: Docker and Kubernetes.&lt;/p&gt;

&lt;p&gt;Docker gives you a single environment in which you have total control over dependencies and versions. And the best part is that the environment will be the same (or very similar) in development and production. We recommend dockerizing every service.&lt;/p&gt;

&lt;p&gt;The other piece of the puzzle is kubernetes. Kubernetes is an open source orchestration system for Docker containers that provides fault tolerance and high availability.
One of the main abstraction in Kubernetes is the Replication Controller. You can specify how many pods (collection of containers) you want to have running and kubernetes will enforce that. Kubernetes performs health checks and automatically restarts the pods if there are errors. Another interesting feature is the ability to increase the number of pods based on load. That way you can scale your service up or down based on load. In this example we can increase the number of task workers if the load is high and reduce it to save costs if the load decreases.&lt;/p&gt;

&lt;p&gt;The other important abstraction is the service. Services provide discoverability and have neat features like automatic load balancing of the traffic between pods. Kubernetes also comes with solutions for logging and monitoring.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;With this we conclude this blog post. We hope the ideas presented here will help you with your own designs. The complete code is up on &lt;a href=&quot;https://github.com/jorgemarsal/webocr/tree/master/webocr&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="tech" /><summary type="html">In this post we’re going to explore how to deliver a modern web service. To make things clearer we’re going to use a OCR service as an example. All the code is available on Github.</summary></entry></feed>